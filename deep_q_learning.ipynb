{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNnR3CBgToFlzody1XR2VHY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Maxime-Bakunzi/deep_q_learning-formative/blob/main/deep_q_learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install gymnasium[atari]\n",
        "!pip install \"autorom[accept-rom-license]\"\n",
        "!pip install ale-py\n",
        "!pip install stable-baselines3"
      ],
      "metadata": {
        "id": "pg-qSA5-33cB",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d039776a-a32a-44b3-ac3c-e68c4c59641e"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: gymnasium[atari] in /usr/local/lib/python3.11/dist-packages (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (3.1.1)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.0.4)\n",
            "Requirement already satisfied: ale-py>=0.9 in /usr/local/lib/python3.11/dist-packages (from gymnasium[atari]) (0.10.2)\n",
            "Requirement already satisfied: autorom[accept-rom-license] in /usr/local/lib/python3.11/dist-packages (0.6.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (8.1.8)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (2.32.3)\n",
            "Requirement already satisfied: AutoROM.accept-rom-license in /usr/local/lib/python3.11/dist-packages (from autorom[accept-rom-license]) (0.6.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->autorom[accept-rom-license]) (2025.1.31)\n",
            "Requirement already satisfied: ale-py in /usr/local/lib/python3.11/dist-packages (0.10.2)\n",
            "Requirement already satisfied: numpy>1.20 in /usr/local/lib/python3.11/dist-packages (from ale-py) (2.0.2)\n",
            "Requirement already satisfied: stable-baselines3 in /usr/local/lib/python3.11/dist-packages (2.5.0)\n",
            "Requirement already satisfied: gymnasium<1.1.0,>=0.29.1 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (1.0.0)\n",
            "Requirement already satisfied: numpy<3.0,>=1.20 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.0.2)\n",
            "Requirement already satisfied: torch<3.0,>=2.3 in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.6.0+cu124)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.1.1)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (2.2.2)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from stable-baselines3) (3.10.0)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (4.12.2)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.11/dist-packages (from gymnasium<1.1.0,>=0.29.1->stable-baselines3) (0.0.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.18.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.4.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2025.3.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.1.0.70 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (9.1.0.70)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.4.5.8 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.5.8)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.2.1.3 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.2.1.3)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.5.147 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (10.3.5.147)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.6.1.9 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (11.6.1.9)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.3.1.170 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.3.1.170)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (0.6.2)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (2.21.5)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (12.4.127)\n",
            "Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (3.2.0)\n",
            "Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch<3.0,>=2.3->stable-baselines3) (1.13.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch<3.0,>=2.3->stable-baselines3) (1.3.0)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.3.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (4.56.0)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (11.1.0)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (3.2.1)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->stable-baselines3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->stable-baselines3) (2025.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->stable-baselines3) (1.17.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch<3.0,>=2.3->stable-baselines3) (3.0.2)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "2qAszxZY0mwP"
      },
      "outputs": [],
      "source": [
        "import gymnasium as gym\n",
        "import numpy as np\n",
        "import ale_py\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper  # For proper Atari pre-processing\n",
        "from stable_baselines3.common.callbacks import BaseCallback"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Custom Callback to Log Training Details and Metrics"
      ],
      "metadata": {
        "id": "FrYNqdN209wy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TrainingLogger(BaseCallback):\n",
        "    def __init__(self, verbose=0):\n",
        "        super(TrainingLogger, self).__init__(verbose)\n",
        "        self.episode_rewards = []\n",
        "        self.episode_lengths = []\n",
        "        self.current_rewards = 0\n",
        "        self.current_length = 0\n",
        "\n",
        "    def _on_step(self) -> bool:\n",
        "        # Increase counters at every step\n",
        "        self.current_length += 1\n",
        "        # Check if 'infos' contains an 'episode' key, then log the metrics\n",
        "        infos = self.locals.get(\"infos\", [])\n",
        "        for info in infos:\n",
        "            if \"episode\" in info:\n",
        "                episode_info = info[\"episode\"]\n",
        "                self.episode_rewards.append(episode_info[\"r\"])\n",
        "                self.episode_lengths.append(episode_info[\"l\"])\n",
        "                print(f\"Episode Reward: {episode_info['r']:.2f} | Length: {episode_info['l']}\")\n",
        "\n",
        "        return True"
      ],
      "metadata": {
        "id": "YIjdeV3A1GI9"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Environment Setup"
      ],
      "metadata": {
        "id": "eljNsyoU1Gvh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Use the Boxing environment with the specified configuration\n",
        "env_id = \"ALE/Boxing-v5\"\n",
        "# For training, we do not need to render (render_mode=None)\n",
        "env = gym.make(env_id, render_mode=None)\n",
        "# Apply the Atari wrappers to preprocess frames (e.g., resizing, frame stacking, etc.)\n",
        "env = AtariWrapper(env)"
      ],
      "metadata": {
        "id": "Fo3b1k3z1QVR"
      },
      "execution_count": 35,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define Hyperparameters"
      ],
      "metadata": {
        "id": "EqthQdYi1QmK"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## I. Hyperparameters"
      ],
      "metadata": {
        "id": "1DmGCHhgWHiR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# These are 1st hyperparameters.\n",
        "learning_rate = 1e-4\n",
        "gamma = 0.99\n",
        "batch_size = 32\n",
        "# Epsilon parameters for exploration in DQN:\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.02\n",
        "epsilon_decay = 1000000  # The number of timesteps over which epsilon decays"
      ],
      "metadata": {
        "id": "vNEBkv-v1szY"
      },
      "execution_count": 36,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "o-XYcQHx2UB9",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "55d2f5a8-b662-42e8-e199-0a01f048ca9d"
      },
      "execution_count": 37,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define the DQN Agent"
      ],
      "metadata": {
        "id": "xRn3BqSA1zKp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting using CNN-based policy.\n",
        "policy = \"CnnPolicy\"\n",
        "\n",
        "model = DQN(\n",
        "    policy,\n",
        "    env,\n",
        "    learning_rate=learning_rate,\n",
        "    gamma=gamma,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    exploration_initial_eps=epsilon_start,\n",
        "    exploration_final_eps=epsilon_end,\n",
        "    # Adjust the exploration fraction to set decay relative to total timesteps\n",
        "    exploration_fraction=epsilon_decay / 1_000_000,\n",
        ")"
      ],
      "metadata": {
        "id": "g2S3zrM91zoY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8aacf0-a58a-4845-e317-697a0ff17489"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Training the Agent\n"
      ],
      "metadata": {
        "id": "TuJ4DR7G62dl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for a total of 500,000 timesteps (adjust based on performance)\n",
        "total_timesteps = 500_000\n",
        "model.learn(total_timesteps=total_timesteps, callback=TrainingLogger())\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/My Drive/deep_q_learning/dqn_model.zip\")\n",
        "print(\"Model saved as dqn_model.zip\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HQ1ytBCx64IO",
        "outputId": "fec9bff2-f0de-44ca-8e01-56d583cd12e9"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1;30;43mStreaming output truncated to the last 5000 lines.\u001b[0m\n",
            "Episode Reward: -21.00 | Length: 444\n",
            "Episode Reward: -9.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -6.06    |\n",
            "|    exploration_rate | 0.927    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 202      |\n",
            "|    total_timesteps  | 37022    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0666   |\n",
            "|    n_updates        | 9230     |\n",
            "----------------------------------\n",
            "Episode Reward: -14.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: -15.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -6.17    |\n",
            "|    exploration_rate | 0.924    |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 212      |\n",
            "|    total_timesteps  | 38785    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0239   |\n",
            "|    n_updates        | 9671     |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "Episode Reward: -16.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -6.13    |\n",
            "|    exploration_rate | 0.921    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 222      |\n",
            "|    total_timesteps  | 40553    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0559   |\n",
            "|    n_updates        | 10113    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -6       |\n",
            "|    exploration_rate | 0.917    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 231      |\n",
            "|    total_timesteps  | 42313    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0354   |\n",
            "|    n_updates        | 10553    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: -8.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.88    |\n",
            "|    exploration_rate | 0.914    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 241      |\n",
            "|    total_timesteps  | 44081    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0303   |\n",
            "|    n_updates        | 10995    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.97    |\n",
            "|    exploration_rate | 0.91     |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 251      |\n",
            "|    total_timesteps  | 45849    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0745   |\n",
            "|    n_updates        | 11437    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.77    |\n",
            "|    exploration_rate | 0.907    |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 261      |\n",
            "|    total_timesteps  | 47618    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0986   |\n",
            "|    n_updates        | 11879    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.56    |\n",
            "|    exploration_rate | 0.903    |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 181      |\n",
            "|    time_elapsed     | 271      |\n",
            "|    total_timesteps  | 49378    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0472   |\n",
            "|    n_updates        | 12319    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.44    |\n",
            "|    exploration_rate | 0.9      |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 181      |\n",
            "|    time_elapsed     | 282      |\n",
            "|    total_timesteps  | 51140    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0507   |\n",
            "|    n_updates        | 12759    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.43    |\n",
            "|    exploration_rate | 0.896    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 181      |\n",
            "|    time_elapsed     | 292      |\n",
            "|    total_timesteps  | 52901    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0572   |\n",
            "|    n_updates        | 13200    |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.56    |\n",
            "|    exploration_rate | 0.893    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 302      |\n",
            "|    total_timesteps  | 54667    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0261   |\n",
            "|    n_updates        | 13641    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: -9.00 | Length: 437\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.31    |\n",
            "|    exploration_rate | 0.889    |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 312      |\n",
            "|    total_timesteps  | 56432    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0461   |\n",
            "|    n_updates        | 14082    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.94    |\n",
            "|    exploration_rate | 0.886    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 322      |\n",
            "|    total_timesteps  | 58192    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0455   |\n",
            "|    n_updates        | 14522    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: -4.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.75    |\n",
            "|    exploration_rate | 0.882    |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 331      |\n",
            "|    total_timesteps  | 59960    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0559   |\n",
            "|    n_updates        | 14964    |\n",
            "----------------------------------\n",
            "Episode Reward: -8.00 | Length: 439\n",
            "Episode Reward: -10.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: -8.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.9     |\n",
            "|    exploration_rate | 0.879    |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 341      |\n",
            "|    total_timesteps  | 61714    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.125    |\n",
            "|    n_updates        | 15403    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: -5.00 | Length: 441\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.84    |\n",
            "|    exploration_rate | 0.876    |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 351      |\n",
            "|    total_timesteps  | 63475    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0645   |\n",
            "|    n_updates        | 15843    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: -13.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.72    |\n",
            "|    exploration_rate | 0.872    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 361      |\n",
            "|    total_timesteps  | 65240    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.066    |\n",
            "|    n_updates        | 16284    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.58    |\n",
            "|    exploration_rate | 0.869    |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 370      |\n",
            "|    total_timesteps  | 67002    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0806   |\n",
            "|    n_updates        | 16725    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: -7.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.44    |\n",
            "|    exploration_rate | 0.865    |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 380      |\n",
            "|    total_timesteps  | 68763    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0571   |\n",
            "|    n_updates        | 17165    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: -8.00 | Length: 438\n",
            "Episode Reward: -12.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.24    |\n",
            "|    exploration_rate | 0.862    |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 390      |\n",
            "|    total_timesteps  | 70527    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0614   |\n",
            "|    n_updates        | 17606    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 445\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.28    |\n",
            "|    exploration_rate | 0.858    |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 400      |\n",
            "|    total_timesteps  | 72293    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0315   |\n",
            "|    n_updates        | 18048    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -12.00 | Length: 438\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.23    |\n",
            "|    exploration_rate | 0.855    |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 410      |\n",
            "|    total_timesteps  | 74057    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0451   |\n",
            "|    n_updates        | 18489    |\n",
            "----------------------------------\n",
            "Episode Reward: -26.00 | Length: 440\n",
            "Episode Reward: -12.00 | Length: 444\n",
            "Episode Reward: -12.00 | Length: 442\n",
            "Episode Reward: -18.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.71    |\n",
            "|    exploration_rate | 0.851    |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 419      |\n",
            "|    total_timesteps  | 75822    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0306   |\n",
            "|    n_updates        | 18930    |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 445\n",
            "Episode Reward: -5.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.66    |\n",
            "|    exploration_rate | 0.848    |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 429      |\n",
            "|    total_timesteps  | 77589    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0599   |\n",
            "|    n_updates        | 19372    |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 443\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: -13.00 | Length: 445\n",
            "Episode Reward: -9.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.61    |\n",
            "|    exploration_rate | 0.844    |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 439      |\n",
            "|    total_timesteps  | 79364    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 19815    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: -15.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "Episode Reward: -9.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.52    |\n",
            "|    exploration_rate | 0.841    |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 449      |\n",
            "|    total_timesteps  | 81130    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0679   |\n",
            "|    n_updates        | 20257    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.26    |\n",
            "|    exploration_rate | 0.838    |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 460      |\n",
            "|    total_timesteps  | 82897    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0465   |\n",
            "|    n_updates        | 20699    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.23    |\n",
            "|    exploration_rate | 0.834    |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 180      |\n",
            "|    time_elapsed     | 470      |\n",
            "|    total_timesteps  | 84660    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0539   |\n",
            "|    n_updates        | 21139    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: 1.00 | Length: 437\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.02    |\n",
            "|    exploration_rate | 0.831    |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 480      |\n",
            "|    total_timesteps  | 86418    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.051    |\n",
            "|    n_updates        | 21579    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 437\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "Episode Reward: -4.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.98    |\n",
            "|    exploration_rate | 0.827    |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 490      |\n",
            "|    total_timesteps  | 88182    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.051    |\n",
            "|    n_updates        | 22020    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.83    |\n",
            "|    exploration_rate | 0.824    |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 500      |\n",
            "|    total_timesteps  | 89946    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0658   |\n",
            "|    n_updates        | 22461    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 445\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.77    |\n",
            "|    exploration_rate | 0.82     |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 510      |\n",
            "|    total_timesteps  | 91721    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0316   |\n",
            "|    n_updates        | 22905    |\n",
            "----------------------------------\n",
            "Episode Reward: -12.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: -11.00 | Length: 441\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.1     |\n",
            "|    exploration_rate | 0.817    |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 521      |\n",
            "|    total_timesteps  | 93491    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0741   |\n",
            "|    n_updates        | 23347    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: -10.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 437\n",
            "Episode Reward: -6.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.14    |\n",
            "|    exploration_rate | 0.813    |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 531      |\n",
            "|    total_timesteps  | 95254    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.116    |\n",
            "|    n_updates        | 23788    |\n",
            "----------------------------------\n",
            "Episode Reward: -15.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.24    |\n",
            "|    exploration_rate | 0.81     |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 541      |\n",
            "|    total_timesteps  | 97026    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0648   |\n",
            "|    n_updates        | 24231    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: -7.00 | Length: 441\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.1     |\n",
            "|    exploration_rate | 0.806    |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 551      |\n",
            "|    total_timesteps  | 98782    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 24670    |\n",
            "----------------------------------\n",
            "Episode Reward: -19.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: -14.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.28    |\n",
            "|    exploration_rate | 0.803    |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 179      |\n",
            "|    time_elapsed     | 561      |\n",
            "|    total_timesteps  | 100546   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0238   |\n",
            "|    n_updates        | 25111    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "Episode Reward: -7.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.44    |\n",
            "|    exploration_rate | 0.799    |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 571      |\n",
            "|    total_timesteps  | 102306   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0477   |\n",
            "|    n_updates        | 25551    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.4     |\n",
            "|    exploration_rate | 0.796    |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 581      |\n",
            "|    total_timesteps  | 104072   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0492   |\n",
            "|    n_updates        | 25992    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "Episode Reward: -4.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.23    |\n",
            "|    exploration_rate | 0.793    |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 592      |\n",
            "|    total_timesteps  | 105837   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0983   |\n",
            "|    n_updates        | 26434    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "Episode Reward: -12.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: -11.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.41    |\n",
            "|    exploration_rate | 0.789    |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 602      |\n",
            "|    total_timesteps  | 107604   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0182   |\n",
            "|    n_updates        | 26875    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: -14.00 | Length: 439\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "Episode Reward: -5.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.42    |\n",
            "|    exploration_rate | 0.786    |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 613      |\n",
            "|    total_timesteps  | 109370   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.033    |\n",
            "|    n_updates        | 27317    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: -15.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.58    |\n",
            "|    exploration_rate | 0.782    |\n",
            "| time/               |          |\n",
            "|    episodes         | 252      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 623      |\n",
            "|    total_timesteps  | 111136   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.029    |\n",
            "|    n_updates        | 27758    |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 443\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: -17.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.87    |\n",
            "|    exploration_rate | 0.779    |\n",
            "| time/               |          |\n",
            "|    episodes         | 256      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 633      |\n",
            "|    total_timesteps  | 112906   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0531   |\n",
            "|    n_updates        | 28201    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.7     |\n",
            "|    exploration_rate | 0.775    |\n",
            "| time/               |          |\n",
            "|    episodes         | 260      |\n",
            "|    fps              | 178      |\n",
            "|    time_elapsed     | 644      |\n",
            "|    total_timesteps  | 114669   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.122    |\n",
            "|    n_updates        | 28642    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.53    |\n",
            "|    exploration_rate | 0.772    |\n",
            "| time/               |          |\n",
            "|    episodes         | 264      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 654      |\n",
            "|    total_timesteps  | 116434   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0931   |\n",
            "|    n_updates        | 29083    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 439\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.39    |\n",
            "|    exploration_rate | 0.768    |\n",
            "| time/               |          |\n",
            "|    episodes         | 268      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 664      |\n",
            "|    total_timesteps  | 118202   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0652   |\n",
            "|    n_updates        | 29525    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.73    |\n",
            "|    exploration_rate | 0.765    |\n",
            "| time/               |          |\n",
            "|    episodes         | 272      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 674      |\n",
            "|    total_timesteps  | 119964   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0697   |\n",
            "|    n_updates        | 29965    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.6     |\n",
            "|    exploration_rate | 0.761    |\n",
            "| time/               |          |\n",
            "|    episodes         | 276      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 684      |\n",
            "|    total_timesteps  | 121726   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0548   |\n",
            "|    n_updates        | 30406    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 437\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.29    |\n",
            "|    exploration_rate | 0.758    |\n",
            "| time/               |          |\n",
            "|    episodes         | 280      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 694      |\n",
            "|    total_timesteps  | 123484   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0216   |\n",
            "|    n_updates        | 30845    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "Episode Reward: -15.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.24    |\n",
            "|    exploration_rate | 0.755    |\n",
            "| time/               |          |\n",
            "|    episodes         | 284      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 704      |\n",
            "|    total_timesteps  | 125247   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0629   |\n",
            "|    n_updates        | 31286    |\n",
            "----------------------------------\n",
            "Episode Reward: -14.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.3     |\n",
            "|    exploration_rate | 0.751    |\n",
            "| time/               |          |\n",
            "|    episodes         | 288      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 714      |\n",
            "|    total_timesteps  | 127003   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0518   |\n",
            "|    n_updates        | 31725    |\n",
            "----------------------------------\n",
            "Episode Reward: -14.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: 7.00 | Length: 439\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.27    |\n",
            "|    exploration_rate | 0.748    |\n",
            "| time/               |          |\n",
            "|    episodes         | 292      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 724      |\n",
            "|    total_timesteps  | 128769   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0509   |\n",
            "|    n_updates        | 32167    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.45    |\n",
            "|    exploration_rate | 0.744    |\n",
            "| time/               |          |\n",
            "|    episodes         | 296      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 734      |\n",
            "|    total_timesteps  | 130534   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0836   |\n",
            "|    n_updates        | 32608    |\n",
            "----------------------------------\n",
            "Episode Reward: -12.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.58    |\n",
            "|    exploration_rate | 0.741    |\n",
            "| time/               |          |\n",
            "|    episodes         | 300      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 744      |\n",
            "|    total_timesteps  | 132294   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.109    |\n",
            "|    n_updates        | 33048    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.59    |\n",
            "|    exploration_rate | 0.737    |\n",
            "| time/               |          |\n",
            "|    episodes         | 304      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 754      |\n",
            "|    total_timesteps  | 134051   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0376   |\n",
            "|    n_updates        | 33487    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 437\n",
            "Episode Reward: 1.00 | Length: 437\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.68    |\n",
            "|    exploration_rate | 0.734    |\n",
            "| time/               |          |\n",
            "|    episodes         | 308      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 764      |\n",
            "|    total_timesteps  | 135809   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0487   |\n",
            "|    n_updates        | 33927    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: -12.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.38    |\n",
            "|    exploration_rate | 0.73     |\n",
            "| time/               |          |\n",
            "|    episodes         | 312      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 774      |\n",
            "|    total_timesteps  | 137570   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0892   |\n",
            "|    n_updates        | 34367    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: 12.00 | Length: 444\n",
            "Episode Reward: -9.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.25    |\n",
            "|    exploration_rate | 0.727    |\n",
            "| time/               |          |\n",
            "|    episodes         | 316      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 784      |\n",
            "|    total_timesteps  | 139335   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.126    |\n",
            "|    n_updates        | 34808    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 437\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.08    |\n",
            "|    exploration_rate | 0.723    |\n",
            "| time/               |          |\n",
            "|    episodes         | 320      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 794      |\n",
            "|    total_timesteps  | 141091   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0706   |\n",
            "|    n_updates        | 35247    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: -5.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.11    |\n",
            "|    exploration_rate | 0.72     |\n",
            "| time/               |          |\n",
            "|    episodes         | 324      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 804      |\n",
            "|    total_timesteps  | 142850   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0706   |\n",
            "|    n_updates        | 35687    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.8     |\n",
            "|    exploration_rate | 0.717    |\n",
            "| time/               |          |\n",
            "|    episodes         | 328      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 814      |\n",
            "|    total_timesteps  | 144616   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.083    |\n",
            "|    n_updates        | 36128    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: 6.00 | Length: 441\n",
            "Episode Reward: -10.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.75    |\n",
            "|    exploration_rate | 0.713    |\n",
            "| time/               |          |\n",
            "|    episodes         | 332      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 824      |\n",
            "|    total_timesteps  | 146388   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0738   |\n",
            "|    n_updates        | 36571    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "Episode Reward: -7.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.72    |\n",
            "|    exploration_rate | 0.71     |\n",
            "| time/               |          |\n",
            "|    episodes         | 336      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 835      |\n",
            "|    total_timesteps  | 148153   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0652   |\n",
            "|    n_updates        | 37013    |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.64    |\n",
            "|    exploration_rate | 0.706    |\n",
            "| time/               |          |\n",
            "|    episodes         | 340      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 845      |\n",
            "|    total_timesteps  | 149915   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0551   |\n",
            "|    n_updates        | 37453    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: -8.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.47    |\n",
            "|    exploration_rate | 0.703    |\n",
            "| time/               |          |\n",
            "|    episodes         | 344      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 855      |\n",
            "|    total_timesteps  | 151681   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0482   |\n",
            "|    n_updates        | 37895    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: -5.00 | Length: 442\n",
            "Episode Reward: 13.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.11    |\n",
            "|    exploration_rate | 0.699    |\n",
            "| time/               |          |\n",
            "|    episodes         | 348      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 865      |\n",
            "|    total_timesteps  | 153448   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.046    |\n",
            "|    n_updates        | 38336    |\n",
            "----------------------------------\n",
            "Episode Reward: -12.00 | Length: 439\n",
            "Episode Reward: -16.00 | Length: 441\n",
            "Episode Reward: -11.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.32    |\n",
            "|    exploration_rate | 0.696    |\n",
            "| time/               |          |\n",
            "|    episodes         | 352      |\n",
            "|    fps              | 177      |\n",
            "|    time_elapsed     | 876      |\n",
            "|    total_timesteps  | 155207   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0833   |\n",
            "|    n_updates        | 38776    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "Episode Reward: -9.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.07    |\n",
            "|    exploration_rate | 0.692    |\n",
            "| time/               |          |\n",
            "|    episodes         | 356      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 886      |\n",
            "|    total_timesteps  | 156966   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0882   |\n",
            "|    n_updates        | 39216    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -7.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.99    |\n",
            "|    exploration_rate | 0.689    |\n",
            "| time/               |          |\n",
            "|    episodes         | 360      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 897      |\n",
            "|    total_timesteps  | 158726   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0841   |\n",
            "|    n_updates        | 39656    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.12    |\n",
            "|    exploration_rate | 0.685    |\n",
            "| time/               |          |\n",
            "|    episodes         | 364      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 907      |\n",
            "|    total_timesteps  | 160491   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0886   |\n",
            "|    n_updates        | 40097    |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.14    |\n",
            "|    exploration_rate | 0.682    |\n",
            "| time/               |          |\n",
            "|    episodes         | 368      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 917      |\n",
            "|    total_timesteps  | 162255   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0872   |\n",
            "|    n_updates        | 40538    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.15    |\n",
            "|    exploration_rate | 0.679    |\n",
            "| time/               |          |\n",
            "|    episodes         | 372      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 927      |\n",
            "|    total_timesteps  | 164017   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0242   |\n",
            "|    n_updates        | 40979    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: -6.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.29    |\n",
            "|    exploration_rate | 0.675    |\n",
            "| time/               |          |\n",
            "|    episodes         | 376      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 937      |\n",
            "|    total_timesteps  | 165782   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0363   |\n",
            "|    n_updates        | 41420    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 445\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.44    |\n",
            "|    exploration_rate | 0.672    |\n",
            "| time/               |          |\n",
            "|    episodes         | 380      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 948      |\n",
            "|    total_timesteps  | 167550   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0435   |\n",
            "|    n_updates        | 41862    |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: -13.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -7.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.37    |\n",
            "|    exploration_rate | 0.668    |\n",
            "| time/               |          |\n",
            "|    episodes         | 384      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 958      |\n",
            "|    total_timesteps  | 169315   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0715   |\n",
            "|    n_updates        | 42303    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 437\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.28    |\n",
            "|    exploration_rate | 0.665    |\n",
            "| time/               |          |\n",
            "|    episodes         | 388      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 969      |\n",
            "|    total_timesteps  | 171071   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0899   |\n",
            "|    n_updates        | 42742    |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: 7.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.1     |\n",
            "|    exploration_rate | 0.661    |\n",
            "| time/               |          |\n",
            "|    episodes         | 392      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 979      |\n",
            "|    total_timesteps  | 172829   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.174    |\n",
            "|    n_updates        | 43182    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: -7.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.03    |\n",
            "|    exploration_rate | 0.658    |\n",
            "| time/               |          |\n",
            "|    episodes         | 396      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 989      |\n",
            "|    total_timesteps  | 174591   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0421   |\n",
            "|    n_updates        | 43622    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.88    |\n",
            "|    exploration_rate | 0.654    |\n",
            "| time/               |          |\n",
            "|    episodes         | 400      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 1000     |\n",
            "|    total_timesteps  | 176357   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 44064    |\n",
            "----------------------------------\n",
            "Episode Reward: -15.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.96    |\n",
            "|    exploration_rate | 0.651    |\n",
            "| time/               |          |\n",
            "|    episodes         | 404      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 1011     |\n",
            "|    total_timesteps  | 178121   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0776   |\n",
            "|    n_updates        | 44505    |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.89    |\n",
            "|    exploration_rate | 0.647    |\n",
            "| time/               |          |\n",
            "|    episodes         | 408      |\n",
            "|    fps              | 176      |\n",
            "|    time_elapsed     | 1021     |\n",
            "|    total_timesteps  | 179882   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0281   |\n",
            "|    n_updates        | 44945    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.88    |\n",
            "|    exploration_rate | 0.644    |\n",
            "| time/               |          |\n",
            "|    episodes         | 412      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1032     |\n",
            "|    total_timesteps  | 181642   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.046    |\n",
            "|    n_updates        | 45385    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.85    |\n",
            "|    exploration_rate | 0.641    |\n",
            "| time/               |          |\n",
            "|    episodes         | 416      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1043     |\n",
            "|    total_timesteps  | 183411   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 45827    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: -20.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "Episode Reward: 10.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.89    |\n",
            "|    exploration_rate | 0.637    |\n",
            "| time/               |          |\n",
            "|    episodes         | 420      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1053     |\n",
            "|    total_timesteps  | 185170   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0815   |\n",
            "|    n_updates        | 46267    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.89    |\n",
            "|    exploration_rate | 0.634    |\n",
            "| time/               |          |\n",
            "|    episodes         | 424      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1064     |\n",
            "|    total_timesteps  | 186941   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0359   |\n",
            "|    n_updates        | 46710    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.85    |\n",
            "|    exploration_rate | 0.63     |\n",
            "| time/               |          |\n",
            "|    episodes         | 428      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1075     |\n",
            "|    total_timesteps  | 188707   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 47151    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.86    |\n",
            "|    exploration_rate | 0.627    |\n",
            "| time/               |          |\n",
            "|    episodes         | 432      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1086     |\n",
            "|    total_timesteps  | 190467   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0814   |\n",
            "|    n_updates        | 47591    |\n",
            "----------------------------------\n",
            "Episode Reward: 8.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.81    |\n",
            "|    exploration_rate | 0.623    |\n",
            "| time/               |          |\n",
            "|    episodes         | 436      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1096     |\n",
            "|    total_timesteps  | 192226   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0886   |\n",
            "|    n_updates        | 48031    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "Episode Reward: -6.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.82    |\n",
            "|    exploration_rate | 0.62     |\n",
            "| time/               |          |\n",
            "|    episodes         | 440      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1107     |\n",
            "|    total_timesteps  | 193993   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0995   |\n",
            "|    n_updates        | 48473    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 445\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.7     |\n",
            "|    exploration_rate | 0.616    |\n",
            "| time/               |          |\n",
            "|    episodes         | 444      |\n",
            "|    fps              | 175      |\n",
            "|    time_elapsed     | 1118     |\n",
            "|    total_timesteps  | 195764   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0479   |\n",
            "|    n_updates        | 48915    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 439\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.86    |\n",
            "|    exploration_rate | 0.613    |\n",
            "| time/               |          |\n",
            "|    episodes         | 448      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1129     |\n",
            "|    total_timesteps  | 197531   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 49357    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.36    |\n",
            "|    exploration_rate | 0.609    |\n",
            "| time/               |          |\n",
            "|    episodes         | 452      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1139     |\n",
            "|    total_timesteps  | 199293   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.054    |\n",
            "|    n_updates        | 49798    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.39    |\n",
            "|    exploration_rate | 0.606    |\n",
            "| time/               |          |\n",
            "|    episodes         | 456      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1150     |\n",
            "|    total_timesteps  | 201061   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0732   |\n",
            "|    n_updates        | 50240    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.33    |\n",
            "|    exploration_rate | 0.602    |\n",
            "| time/               |          |\n",
            "|    episodes         | 460      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1161     |\n",
            "|    total_timesteps  | 202820   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0561   |\n",
            "|    n_updates        | 50679    |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: 11.00 | Length: 437\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -1.14    |\n",
            "|    exploration_rate | 0.599    |\n",
            "| time/               |          |\n",
            "|    episodes         | 464      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1171     |\n",
            "|    total_timesteps  | 204575   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 51118    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.88    |\n",
            "|    exploration_rate | 0.596    |\n",
            "| time/               |          |\n",
            "|    episodes         | 468      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1182     |\n",
            "|    total_timesteps  | 206338   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0296   |\n",
            "|    n_updates        | 51559    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.81    |\n",
            "|    exploration_rate | 0.592    |\n",
            "| time/               |          |\n",
            "|    episodes         | 472      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1193     |\n",
            "|    total_timesteps  | 208111   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0881   |\n",
            "|    n_updates        | 52002    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 437\n",
            "Episode Reward: -6.00 | Length: 440\n",
            "Episode Reward: -8.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.82    |\n",
            "|    exploration_rate | 0.589    |\n",
            "| time/               |          |\n",
            "|    episodes         | 476      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1203     |\n",
            "|    total_timesteps  | 209865   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.084    |\n",
            "|    n_updates        | 52441    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.71    |\n",
            "|    exploration_rate | 0.585    |\n",
            "| time/               |          |\n",
            "|    episodes         | 480      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1214     |\n",
            "|    total_timesteps  | 211630   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0822   |\n",
            "|    n_updates        | 52882    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.57    |\n",
            "|    exploration_rate | 0.582    |\n",
            "| time/               |          |\n",
            "|    episodes         | 484      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1225     |\n",
            "|    total_timesteps  | 213393   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.191    |\n",
            "|    n_updates        | 53323    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: -9.00 | Length: 442\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.41    |\n",
            "|    exploration_rate | 0.578    |\n",
            "| time/               |          |\n",
            "|    episodes         | 488      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1235     |\n",
            "|    total_timesteps  | 215163   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 53765    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 445\n",
            "Episode Reward: -4.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.46    |\n",
            "|    exploration_rate | 0.575    |\n",
            "| time/               |          |\n",
            "|    episodes         | 492      |\n",
            "|    fps              | 174      |\n",
            "|    time_elapsed     | 1246     |\n",
            "|    total_timesteps  | 216927   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0633   |\n",
            "|    n_updates        | 54206    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: -7.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.45    |\n",
            "|    exploration_rate | 0.571    |\n",
            "| time/               |          |\n",
            "|    episodes         | 496      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1257     |\n",
            "|    total_timesteps  | 218688   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0688   |\n",
            "|    n_updates        | 54646    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: -8.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 445\n",
            "Episode Reward: 8.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.34    |\n",
            "|    exploration_rate | 0.568    |\n",
            "| time/               |          |\n",
            "|    episodes         | 500      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1267     |\n",
            "|    total_timesteps  | 220455   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0677   |\n",
            "|    n_updates        | 55088    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.17    |\n",
            "|    exploration_rate | 0.564    |\n",
            "| time/               |          |\n",
            "|    episodes         | 504      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1278     |\n",
            "|    total_timesteps  | 222212   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0805   |\n",
            "|    n_updates        | 55527    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 441\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.06    |\n",
            "|    exploration_rate | 0.561    |\n",
            "| time/               |          |\n",
            "|    episodes         | 508      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1289     |\n",
            "|    total_timesteps  | 223977   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0782   |\n",
            "|    n_updates        | 55969    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 442\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.06     |\n",
            "|    exploration_rate | 0.558    |\n",
            "| time/               |          |\n",
            "|    episodes         | 512      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1300     |\n",
            "|    total_timesteps  | 225747   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 56411    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -0.15    |\n",
            "|    exploration_rate | 0.554    |\n",
            "| time/               |          |\n",
            "|    episodes         | 516      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1310     |\n",
            "|    total_timesteps  | 227508   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.039    |\n",
            "|    n_updates        | 56851    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 445\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: 2.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.01     |\n",
            "|    exploration_rate | 0.551    |\n",
            "| time/               |          |\n",
            "|    episodes         | 520      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1321     |\n",
            "|    total_timesteps  | 229271   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0654   |\n",
            "|    n_updates        | 57292    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "Episode Reward: -5.00 | Length: 437\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0        |\n",
            "|    exploration_rate | 0.547    |\n",
            "| time/               |          |\n",
            "|    episodes         | 524      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1332     |\n",
            "|    total_timesteps  | 231038   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0636   |\n",
            "|    n_updates        | 57734    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 437\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.02     |\n",
            "|    exploration_rate | 0.544    |\n",
            "| time/               |          |\n",
            "|    episodes         | 528      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1342     |\n",
            "|    total_timesteps  | 232801   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.138    |\n",
            "|    n_updates        | 58175    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: -17.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.1      |\n",
            "|    exploration_rate | 0.54     |\n",
            "| time/               |          |\n",
            "|    episodes         | 532      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1353     |\n",
            "|    total_timesteps  | 234558   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0992   |\n",
            "|    n_updates        | 58614    |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 438\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.24     |\n",
            "|    exploration_rate | 0.537    |\n",
            "| time/               |          |\n",
            "|    episodes         | 536      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1364     |\n",
            "|    total_timesteps  | 236323   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0978   |\n",
            "|    n_updates        | 59055    |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.42     |\n",
            "|    exploration_rate | 0.533    |\n",
            "| time/               |          |\n",
            "|    episodes         | 540      |\n",
            "|    fps              | 173      |\n",
            "|    time_elapsed     | 1375     |\n",
            "|    total_timesteps  | 238089   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0627   |\n",
            "|    n_updates        | 59497    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 441\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.49     |\n",
            "|    exploration_rate | 0.53     |\n",
            "| time/               |          |\n",
            "|    episodes         | 544      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1386     |\n",
            "|    total_timesteps  | 239852   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0902   |\n",
            "|    n_updates        | 59937    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.64     |\n",
            "|    exploration_rate | 0.526    |\n",
            "| time/               |          |\n",
            "|    episodes         | 548      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1397     |\n",
            "|    total_timesteps  | 241616   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0634   |\n",
            "|    n_updates        | 60378    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.53     |\n",
            "|    exploration_rate | 0.523    |\n",
            "| time/               |          |\n",
            "|    episodes         | 552      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1409     |\n",
            "|    total_timesteps  | 243383   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0422   |\n",
            "|    n_updates        | 60820    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -17.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: -5.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.5      |\n",
            "|    exploration_rate | 0.52     |\n",
            "| time/               |          |\n",
            "|    episodes         | 556      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1420     |\n",
            "|    total_timesteps  | 245140   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0928   |\n",
            "|    n_updates        | 61259    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.41     |\n",
            "|    exploration_rate | 0.516    |\n",
            "| time/               |          |\n",
            "|    episodes         | 560      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1431     |\n",
            "|    total_timesteps  | 246907   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.076    |\n",
            "|    n_updates        | 61701    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.33     |\n",
            "|    exploration_rate | 0.513    |\n",
            "| time/               |          |\n",
            "|    episodes         | 564      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1443     |\n",
            "|    total_timesteps  | 248667   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.112    |\n",
            "|    n_updates        | 62141    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.16     |\n",
            "|    exploration_rate | 0.509    |\n",
            "| time/               |          |\n",
            "|    episodes         | 568      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1454     |\n",
            "|    total_timesteps  | 250433   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0489   |\n",
            "|    n_updates        | 62583    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 437\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.18     |\n",
            "|    exploration_rate | 0.506    |\n",
            "| time/               |          |\n",
            "|    episodes         | 572      |\n",
            "|    fps              | 172      |\n",
            "|    time_elapsed     | 1465     |\n",
            "|    total_timesteps  | 252194   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.034    |\n",
            "|    n_updates        | 63023    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 445\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.28     |\n",
            "|    exploration_rate | 0.502    |\n",
            "| time/               |          |\n",
            "|    episodes         | 576      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1477     |\n",
            "|    total_timesteps  | 253961   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0887   |\n",
            "|    n_updates        | 63465    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 445\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.32     |\n",
            "|    exploration_rate | 0.499    |\n",
            "| time/               |          |\n",
            "|    episodes         | 580      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1488     |\n",
            "|    total_timesteps  | 255726   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 63906    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: 7.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.36     |\n",
            "|    exploration_rate | 0.495    |\n",
            "| time/               |          |\n",
            "|    episodes         | 584      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1500     |\n",
            "|    total_timesteps  | 257491   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 64347    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.25     |\n",
            "|    exploration_rate | 0.492    |\n",
            "| time/               |          |\n",
            "|    episodes         | 588      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1511     |\n",
            "|    total_timesteps  | 259259   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0759   |\n",
            "|    n_updates        | 64789    |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 443\n",
            "Episode Reward: 10.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.41     |\n",
            "|    exploration_rate | 0.488    |\n",
            "| time/               |          |\n",
            "|    episodes         | 592      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1522     |\n",
            "|    total_timesteps  | 261022   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.27     |\n",
            "|    n_updates        | 65230    |\n",
            "----------------------------------\n",
            "Episode Reward: 9.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.39     |\n",
            "|    exploration_rate | 0.485    |\n",
            "| time/               |          |\n",
            "|    episodes         | 596      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1533     |\n",
            "|    total_timesteps  | 262776   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0693   |\n",
            "|    n_updates        | 65668    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 443\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.48     |\n",
            "|    exploration_rate | 0.481    |\n",
            "| time/               |          |\n",
            "|    episodes         | 600      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1545     |\n",
            "|    total_timesteps  | 264546   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0397   |\n",
            "|    n_updates        | 66111    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: 12.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.59     |\n",
            "|    exploration_rate | 0.478    |\n",
            "| time/               |          |\n",
            "|    episodes         | 604      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1556     |\n",
            "|    total_timesteps  | 266305   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.15     |\n",
            "|    n_updates        | 66551    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 445\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.63     |\n",
            "|    exploration_rate | 0.475    |\n",
            "| time/               |          |\n",
            "|    episodes         | 608      |\n",
            "|    fps              | 171      |\n",
            "|    time_elapsed     | 1567     |\n",
            "|    total_timesteps  | 268077   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0548   |\n",
            "|    n_updates        | 66994    |\n",
            "----------------------------------\n",
            "Episode Reward: -19.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: -10.00 | Length: 442\n",
            "Episode Reward: -9.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.12     |\n",
            "|    exploration_rate | 0.471    |\n",
            "| time/               |          |\n",
            "|    episodes         | 612      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1578     |\n",
            "|    total_timesteps  | 269849   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0587   |\n",
            "|    n_updates        | 67437    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 10.00 | Length: 439\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.5      |\n",
            "|    exploration_rate | 0.468    |\n",
            "| time/               |          |\n",
            "|    episodes         | 616      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1589     |\n",
            "|    total_timesteps  | 271611   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0766   |\n",
            "|    n_updates        | 67877    |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.53     |\n",
            "|    exploration_rate | 0.464    |\n",
            "| time/               |          |\n",
            "|    episodes         | 620      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1600     |\n",
            "|    total_timesteps  | 273371   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0751   |\n",
            "|    n_updates        | 68317    |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: 2.00 | Length: 445\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.72     |\n",
            "|    exploration_rate | 0.461    |\n",
            "| time/               |          |\n",
            "|    episodes         | 624      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1611     |\n",
            "|    total_timesteps  | 275142   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0777   |\n",
            "|    n_updates        | 68760    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.94     |\n",
            "|    exploration_rate | 0.457    |\n",
            "| time/               |          |\n",
            "|    episodes         | 628      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1622     |\n",
            "|    total_timesteps  | 276896   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0894   |\n",
            "|    n_updates        | 69198    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: -17.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.65     |\n",
            "|    exploration_rate | 0.454    |\n",
            "| time/               |          |\n",
            "|    episodes         | 632      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1632     |\n",
            "|    total_timesteps  | 278652   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.247    |\n",
            "|    n_updates        | 69637    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 445\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.65     |\n",
            "|    exploration_rate | 0.45     |\n",
            "| time/               |          |\n",
            "|    episodes         | 636      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1643     |\n",
            "|    total_timesteps  | 280425   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.228    |\n",
            "|    n_updates        | 70081    |\n",
            "----------------------------------\n",
            "Episode Reward: 8.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.63     |\n",
            "|    exploration_rate | 0.447    |\n",
            "| time/               |          |\n",
            "|    episodes         | 640      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1654     |\n",
            "|    total_timesteps  | 282181   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 70520    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.64     |\n",
            "|    exploration_rate | 0.443    |\n",
            "| time/               |          |\n",
            "|    episodes         | 644      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1664     |\n",
            "|    total_timesteps  | 283946   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0753   |\n",
            "|    n_updates        | 70961    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.63     |\n",
            "|    exploration_rate | 0.44     |\n",
            "| time/               |          |\n",
            "|    episodes         | 648      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1675     |\n",
            "|    total_timesteps  | 285713   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 71403    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: 9.00 | Length: 441\n",
            "Episode Reward: -17.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.68     |\n",
            "|    exploration_rate | 0.437    |\n",
            "| time/               |          |\n",
            "|    episodes         | 652      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1686     |\n",
            "|    total_timesteps  | 287479   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0806   |\n",
            "|    n_updates        | 71844    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 439\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 0.99     |\n",
            "|    exploration_rate | 0.433    |\n",
            "| time/               |          |\n",
            "|    episodes         | 656      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1696     |\n",
            "|    total_timesteps  | 289238   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0616   |\n",
            "|    n_updates        | 72284    |\n",
            "----------------------------------\n",
            "Episode Reward: 9.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.25     |\n",
            "|    exploration_rate | 0.43     |\n",
            "| time/               |          |\n",
            "|    episodes         | 660      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1707     |\n",
            "|    total_timesteps  | 291001   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0592   |\n",
            "|    n_updates        | 72725    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.38     |\n",
            "|    exploration_rate | 0.426    |\n",
            "| time/               |          |\n",
            "|    episodes         | 664      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1718     |\n",
            "|    total_timesteps  | 292768   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0806   |\n",
            "|    n_updates        | 73166    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.59     |\n",
            "|    exploration_rate | 0.423    |\n",
            "| time/               |          |\n",
            "|    episodes         | 668      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1728     |\n",
            "|    total_timesteps  | 294527   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0388   |\n",
            "|    n_updates        | 73606    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 437\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.5      |\n",
            "|    exploration_rate | 0.419    |\n",
            "| time/               |          |\n",
            "|    episodes         | 672      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1739     |\n",
            "|    total_timesteps  | 296287   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0716   |\n",
            "|    n_updates        | 74046    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.58     |\n",
            "|    exploration_rate | 0.416    |\n",
            "| time/               |          |\n",
            "|    episodes         | 676      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1750     |\n",
            "|    total_timesteps  | 298048   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0617   |\n",
            "|    n_updates        | 74486    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.61     |\n",
            "|    exploration_rate | 0.412    |\n",
            "| time/               |          |\n",
            "|    episodes         | 680      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1761     |\n",
            "|    total_timesteps  | 299811   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.079    |\n",
            "|    n_updates        | 74927    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: -9.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.54     |\n",
            "|    exploration_rate | 0.409    |\n",
            "| time/               |          |\n",
            "|    episodes         | 684      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1772     |\n",
            "|    total_timesteps  | 301578   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0795   |\n",
            "|    n_updates        | 75369    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 445\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.64     |\n",
            "|    exploration_rate | 0.405    |\n",
            "| time/               |          |\n",
            "|    episodes         | 688      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1782     |\n",
            "|    total_timesteps  | 303347   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 75811    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: 8.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.57     |\n",
            "|    exploration_rate | 0.402    |\n",
            "| time/               |          |\n",
            "|    episodes         | 692      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1793     |\n",
            "|    total_timesteps  | 305112   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0584   |\n",
            "|    n_updates        | 76252    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: 10.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.64     |\n",
            "|    exploration_rate | 0.399    |\n",
            "| time/               |          |\n",
            "|    episodes         | 696      |\n",
            "|    fps              | 170      |\n",
            "|    time_elapsed     | 1804     |\n",
            "|    total_timesteps  | 306871   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0848   |\n",
            "|    n_updates        | 76692    |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 443\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 11.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.8      |\n",
            "|    exploration_rate | 0.395    |\n",
            "| time/               |          |\n",
            "|    episodes         | 700      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1815     |\n",
            "|    total_timesteps  | 308642   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0694   |\n",
            "|    n_updates        | 77135    |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.75     |\n",
            "|    exploration_rate | 0.392    |\n",
            "| time/               |          |\n",
            "|    episodes         | 704      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1826     |\n",
            "|    total_timesteps  | 310406   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0695   |\n",
            "|    n_updates        | 77576    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.69     |\n",
            "|    exploration_rate | 0.388    |\n",
            "| time/               |          |\n",
            "|    episodes         | 708      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1837     |\n",
            "|    total_timesteps  | 312174   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0713   |\n",
            "|    n_updates        | 78018    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: -12.00 | Length: 439\n",
            "Episode Reward: 9.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.16     |\n",
            "|    exploration_rate | 0.385    |\n",
            "| time/               |          |\n",
            "|    episodes         | 712      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1847     |\n",
            "|    total_timesteps  | 313934   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0829   |\n",
            "|    n_updates        | 78458    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.97     |\n",
            "|    exploration_rate | 0.381    |\n",
            "| time/               |          |\n",
            "|    episodes         | 716      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1858     |\n",
            "|    total_timesteps  | 315703   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0649   |\n",
            "|    n_updates        | 78900    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.91     |\n",
            "|    exploration_rate | 0.378    |\n",
            "| time/               |          |\n",
            "|    episodes         | 720      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1869     |\n",
            "|    total_timesteps  | 317459   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.189    |\n",
            "|    n_updates        | 79339    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 445\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.85     |\n",
            "|    exploration_rate | 0.374    |\n",
            "| time/               |          |\n",
            "|    episodes         | 724      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1879     |\n",
            "|    total_timesteps  | 319228   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0823   |\n",
            "|    n_updates        | 79781    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 1.7      |\n",
            "|    exploration_rate | 0.371    |\n",
            "| time/               |          |\n",
            "|    episodes         | 728      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1890     |\n",
            "|    total_timesteps  | 320985   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.114    |\n",
            "|    n_updates        | 80221    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.1      |\n",
            "|    exploration_rate | 0.367    |\n",
            "| time/               |          |\n",
            "|    episodes         | 732      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1900     |\n",
            "|    total_timesteps  | 322747   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0797   |\n",
            "|    n_updates        | 80661    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.11     |\n",
            "|    exploration_rate | 0.364    |\n",
            "| time/               |          |\n",
            "|    episodes         | 736      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1911     |\n",
            "|    total_timesteps  | 324518   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.137    |\n",
            "|    n_updates        | 81104    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.17     |\n",
            "|    exploration_rate | 0.36     |\n",
            "| time/               |          |\n",
            "|    episodes         | 740      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1921     |\n",
            "|    total_timesteps  | 326282   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.088    |\n",
            "|    n_updates        | 81545    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.12     |\n",
            "|    exploration_rate | 0.357    |\n",
            "| time/               |          |\n",
            "|    episodes         | 744      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1932     |\n",
            "|    total_timesteps  | 328049   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0574   |\n",
            "|    n_updates        | 81987    |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.18     |\n",
            "|    exploration_rate | 0.354    |\n",
            "| time/               |          |\n",
            "|    episodes         | 748      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1942     |\n",
            "|    total_timesteps  | 329810   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.141    |\n",
            "|    n_updates        | 82427    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 445\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.29     |\n",
            "|    exploration_rate | 0.35     |\n",
            "| time/               |          |\n",
            "|    episodes         | 752      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1953     |\n",
            "|    total_timesteps  | 331578   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 82869    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: 5.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.27     |\n",
            "|    exploration_rate | 0.347    |\n",
            "| time/               |          |\n",
            "|    episodes         | 756      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1964     |\n",
            "|    total_timesteps  | 333341   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 83310    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: 14.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.24     |\n",
            "|    exploration_rate | 0.343    |\n",
            "| time/               |          |\n",
            "|    episodes         | 760      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1974     |\n",
            "|    total_timesteps  | 335103   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 83750    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.25     |\n",
            "|    exploration_rate | 0.34     |\n",
            "| time/               |          |\n",
            "|    episodes         | 764      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1985     |\n",
            "|    total_timesteps  | 336860   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 84189    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "Episode Reward: 14.00 | Length: 444\n",
            "Episode Reward: 9.00 | Length: 444\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.38     |\n",
            "|    exploration_rate | 0.336    |\n",
            "| time/               |          |\n",
            "|    episodes         | 768      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 1996     |\n",
            "|    total_timesteps  | 338628   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 84631    |\n",
            "----------------------------------\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: 10.00 | Length: 437\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.67     |\n",
            "|    exploration_rate | 0.333    |\n",
            "| time/               |          |\n",
            "|    episodes         | 772      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2007     |\n",
            "|    total_timesteps  | 340389   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0804   |\n",
            "|    n_updates        | 85072    |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.65     |\n",
            "|    exploration_rate | 0.329    |\n",
            "| time/               |          |\n",
            "|    episodes         | 776      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2018     |\n",
            "|    total_timesteps  | 342156   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.173    |\n",
            "|    n_updates        | 85513    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 2.76     |\n",
            "|    exploration_rate | 0.326    |\n",
            "| time/               |          |\n",
            "|    episodes         | 780      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2028     |\n",
            "|    total_timesteps  | 343923   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0553   |\n",
            "|    n_updates        | 85955    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: 14.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.04     |\n",
            "|    exploration_rate | 0.322    |\n",
            "| time/               |          |\n",
            "|    episodes         | 784      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2039     |\n",
            "|    total_timesteps  | 345680   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.114    |\n",
            "|    n_updates        | 86394    |\n",
            "----------------------------------\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: 14.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.05     |\n",
            "|    exploration_rate | 0.319    |\n",
            "| time/               |          |\n",
            "|    episodes         | 788      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2050     |\n",
            "|    total_timesteps  | 347447   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.131    |\n",
            "|    n_updates        | 86836    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.16     |\n",
            "|    exploration_rate | 0.316    |\n",
            "| time/               |          |\n",
            "|    episodes         | 792      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2060     |\n",
            "|    total_timesteps  | 349212   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.161    |\n",
            "|    n_updates        | 87277    |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 443\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.29     |\n",
            "|    exploration_rate | 0.312    |\n",
            "| time/               |          |\n",
            "|    episodes         | 796      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2071     |\n",
            "|    total_timesteps  | 350977   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0755   |\n",
            "|    n_updates        | 87719    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: 14.00 | Length: 438\n",
            "Episode Reward: 9.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.28     |\n",
            "|    exploration_rate | 0.309    |\n",
            "| time/               |          |\n",
            "|    episodes         | 800      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2082     |\n",
            "|    total_timesteps  | 352734   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 88158    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: 10.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.39     |\n",
            "|    exploration_rate | 0.305    |\n",
            "| time/               |          |\n",
            "|    episodes         | 804      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2093     |\n",
            "|    total_timesteps  | 354506   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0633   |\n",
            "|    n_updates        | 88601    |\n",
            "----------------------------------\n",
            "Episode Reward: 17.00 | Length: 445\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.61     |\n",
            "|    exploration_rate | 0.302    |\n",
            "| time/               |          |\n",
            "|    episodes         | 808      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2104     |\n",
            "|    total_timesteps  | 356271   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.11     |\n",
            "|    n_updates        | 89042    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.61     |\n",
            "|    exploration_rate | 0.298    |\n",
            "| time/               |          |\n",
            "|    episodes         | 812      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2115     |\n",
            "|    total_timesteps  | 358037   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0586   |\n",
            "|    n_updates        | 89484    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 9.00 | Length: 441\n",
            "Episode Reward: 9.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.85     |\n",
            "|    exploration_rate | 0.295    |\n",
            "| time/               |          |\n",
            "|    episodes         | 816      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2126     |\n",
            "|    total_timesteps  | 359805   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 89926    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 3.91     |\n",
            "|    exploration_rate | 0.291    |\n",
            "| time/               |          |\n",
            "|    episodes         | 820      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2137     |\n",
            "|    total_timesteps  | 361575   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0674   |\n",
            "|    n_updates        | 90368    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 9.00 | Length: 442\n",
            "Episode Reward: 15.00 | Length: 429\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.22     |\n",
            "|    exploration_rate | 0.288    |\n",
            "| time/               |          |\n",
            "|    episodes         | 824      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2147     |\n",
            "|    total_timesteps  | 363330   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.107    |\n",
            "|    n_updates        | 90807    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: 12.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 445\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.36     |\n",
            "|    exploration_rate | 0.284    |\n",
            "| time/               |          |\n",
            "|    episodes         | 828      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2158     |\n",
            "|    total_timesteps  | 365100   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0526   |\n",
            "|    n_updates        | 91249    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.31     |\n",
            "|    exploration_rate | 0.281    |\n",
            "| time/               |          |\n",
            "|    episodes         | 832      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2169     |\n",
            "|    total_timesteps  | 366865   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0881   |\n",
            "|    n_updates        | 91691    |\n",
            "----------------------------------\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "Episode Reward: 9.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.34     |\n",
            "|    exploration_rate | 0.277    |\n",
            "| time/               |          |\n",
            "|    episodes         | 836      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2180     |\n",
            "|    total_timesteps  | 368626   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 92131    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: 22.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 5.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.51     |\n",
            "|    exploration_rate | 0.274    |\n",
            "| time/               |          |\n",
            "|    episodes         | 840      |\n",
            "|    fps              | 169      |\n",
            "|    time_elapsed     | 2191     |\n",
            "|    total_timesteps  | 370384   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.159    |\n",
            "|    n_updates        | 92570    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 437\n",
            "Episode Reward: 7.00 | Length: 439\n",
            "Episode Reward: 6.00 | Length: 443\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.58     |\n",
            "|    exploration_rate | 0.271    |\n",
            "| time/               |          |\n",
            "|    episodes         | 844      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2202     |\n",
            "|    total_timesteps  | 372141   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0468   |\n",
            "|    n_updates        | 93010    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: 11.00 | Length: 440\n",
            "Episode Reward: 9.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.6      |\n",
            "|    exploration_rate | 0.267    |\n",
            "| time/               |          |\n",
            "|    episodes         | 848      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2213     |\n",
            "|    total_timesteps  | 373905   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0528   |\n",
            "|    n_updates        | 93451    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.64     |\n",
            "|    exploration_rate | 0.264    |\n",
            "| time/               |          |\n",
            "|    episodes         | 852      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2224     |\n",
            "|    total_timesteps  | 375670   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0413   |\n",
            "|    n_updates        | 93892    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "Episode Reward: 12.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: 14.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.77     |\n",
            "|    exploration_rate | 0.26     |\n",
            "| time/               |          |\n",
            "|    episodes         | 856      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2234     |\n",
            "|    total_timesteps  | 377439   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 94334    |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 445\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.79     |\n",
            "|    exploration_rate | 0.257    |\n",
            "| time/               |          |\n",
            "|    episodes         | 860      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2245     |\n",
            "|    total_timesteps  | 379212   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0529   |\n",
            "|    n_updates        | 94777    |\n",
            "----------------------------------\n",
            "Episode Reward: 11.00 | Length: 437\n",
            "Episode Reward: 5.00 | Length: 445\n",
            "Episode Reward: 13.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.95     |\n",
            "|    exploration_rate | 0.253    |\n",
            "| time/               |          |\n",
            "|    episodes         | 864      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2255     |\n",
            "|    total_timesteps  | 380976   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0769   |\n",
            "|    n_updates        | 95218    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 11.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.81     |\n",
            "|    exploration_rate | 0.25     |\n",
            "| time/               |          |\n",
            "|    episodes         | 868      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2266     |\n",
            "|    total_timesteps  | 382741   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.142    |\n",
            "|    n_updates        | 95660    |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: 9.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.57     |\n",
            "|    exploration_rate | 0.246    |\n",
            "| time/               |          |\n",
            "|    episodes         | 872      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2277     |\n",
            "|    total_timesteps  | 384497   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.047    |\n",
            "|    n_updates        | 96099    |\n",
            "----------------------------------\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 443\n",
            "Episode Reward: 7.00 | Length: 443\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.76     |\n",
            "|    exploration_rate | 0.243    |\n",
            "| time/               |          |\n",
            "|    episodes         | 876      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2288     |\n",
            "|    total_timesteps  | 386267   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 96541    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.84     |\n",
            "|    exploration_rate | 0.239    |\n",
            "| time/               |          |\n",
            "|    episodes         | 880      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2299     |\n",
            "|    total_timesteps  | 388022   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.13     |\n",
            "|    n_updates        | 96980    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 445\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.69     |\n",
            "|    exploration_rate | 0.236    |\n",
            "| time/               |          |\n",
            "|    episodes         | 884      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2310     |\n",
            "|    total_timesteps  | 389789   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.352    |\n",
            "|    n_updates        | 97422    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.233    |\n",
            "| time/               |          |\n",
            "|    episodes         | 888      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2320     |\n",
            "|    total_timesteps  | 391545   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0723   |\n",
            "|    n_updates        | 97861    |\n",
            "----------------------------------\n",
            "Episode Reward: 15.00 | Length: 439\n",
            "Episode Reward: 14.00 | Length: 438\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.82     |\n",
            "|    exploration_rate | 0.229    |\n",
            "| time/               |          |\n",
            "|    episodes         | 892      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2331     |\n",
            "|    total_timesteps  | 393303   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0641   |\n",
            "|    n_updates        | 98300    |\n",
            "----------------------------------\n",
            "Episode Reward: 14.00 | Length: 443\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.87     |\n",
            "|    exploration_rate | 0.226    |\n",
            "| time/               |          |\n",
            "|    episodes         | 896      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2341     |\n",
            "|    total_timesteps  | 395067   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.129    |\n",
            "|    n_updates        | 98741    |\n",
            "----------------------------------\n",
            "Episode Reward: 11.00 | Length: 443\n",
            "Episode Reward: 9.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "Episode Reward: 15.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.96     |\n",
            "|    exploration_rate | 0.222    |\n",
            "| time/               |          |\n",
            "|    episodes         | 900      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2352     |\n",
            "|    total_timesteps  | 396842   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.077    |\n",
            "|    n_updates        | 99185    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.75     |\n",
            "|    exploration_rate | 0.219    |\n",
            "| time/               |          |\n",
            "|    episodes         | 904      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2363     |\n",
            "|    total_timesteps  | 398613   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0783   |\n",
            "|    n_updates        | 99628    |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 6.00 | Length: 443\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.7      |\n",
            "|    exploration_rate | 0.215    |\n",
            "| time/               |          |\n",
            "|    episodes         | 908      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2373     |\n",
            "|    total_timesteps  | 400378   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 100069   |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: 10.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.81     |\n",
            "|    exploration_rate | 0.212    |\n",
            "| time/               |          |\n",
            "|    episodes         | 912      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2384     |\n",
            "|    total_timesteps  | 402147   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.156    |\n",
            "|    n_updates        | 100511   |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 13.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: 14.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.86     |\n",
            "|    exploration_rate | 0.208    |\n",
            "| time/               |          |\n",
            "|    episodes         | 916      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2395     |\n",
            "|    total_timesteps  | 403914   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0862   |\n",
            "|    n_updates        | 100953   |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.71     |\n",
            "|    exploration_rate | 0.205    |\n",
            "| time/               |          |\n",
            "|    episodes         | 920      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2406     |\n",
            "|    total_timesteps  | 405677   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 101394   |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: 8.00 | Length: 440\n",
            "Episode Reward: 18.00 | Length: 444\n",
            "Episode Reward: 5.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.65     |\n",
            "|    exploration_rate | 0.201    |\n",
            "| time/               |          |\n",
            "|    episodes         | 924      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2417     |\n",
            "|    total_timesteps  | 407439   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.12     |\n",
            "|    n_updates        | 101834   |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: 8.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.63     |\n",
            "|    exploration_rate | 0.198    |\n",
            "| time/               |          |\n",
            "|    episodes         | 928      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2428     |\n",
            "|    total_timesteps  | 409198   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0656   |\n",
            "|    n_updates        | 102274   |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 16.00 | Length: 428\n",
            "Episode Reward: 14.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.88     |\n",
            "|    exploration_rate | 0.195    |\n",
            "| time/               |          |\n",
            "|    episodes         | 932      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2439     |\n",
            "|    total_timesteps  | 410947   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.224    |\n",
            "|    n_updates        | 102711   |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "Episode Reward: 9.00 | Length: 439\n",
            "Episode Reward: 6.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.88     |\n",
            "|    exploration_rate | 0.191    |\n",
            "| time/               |          |\n",
            "|    episodes         | 936      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2450     |\n",
            "|    total_timesteps  | 412711   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0914   |\n",
            "|    n_updates        | 103152   |\n",
            "----------------------------------\n",
            "Episode Reward: 18.00 | Length: 438\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 4.86     |\n",
            "|    exploration_rate | 0.188    |\n",
            "| time/               |          |\n",
            "|    episodes         | 940      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2461     |\n",
            "|    total_timesteps  | 414474   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.123    |\n",
            "|    n_updates        | 103593   |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "Episode Reward: 9.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: 16.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.1      |\n",
            "|    exploration_rate | 0.184    |\n",
            "| time/               |          |\n",
            "|    episodes         | 944      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2472     |\n",
            "|    total_timesteps  | 416236   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0668   |\n",
            "|    n_updates        | 104033   |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 439\n",
            "Episode Reward: 9.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.07     |\n",
            "|    exploration_rate | 0.181    |\n",
            "| time/               |          |\n",
            "|    episodes         | 948      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2483     |\n",
            "|    total_timesteps  | 417995   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.385    |\n",
            "|    n_updates        | 104473   |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 25.00 | Length: 420\n",
            "Episode Reward: 15.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.33     |\n",
            "|    exploration_rate | 0.177    |\n",
            "| time/               |          |\n",
            "|    episodes         | 952      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2494     |\n",
            "|    total_timesteps  | 419736   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0711   |\n",
            "|    n_updates        | 104908   |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: 11.00 | Length: 439\n",
            "Episode Reward: 10.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.38     |\n",
            "|    exploration_rate | 0.174    |\n",
            "| time/               |          |\n",
            "|    episodes         | 956      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2505     |\n",
            "|    total_timesteps  | 421500   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.114    |\n",
            "|    n_updates        | 105349   |\n",
            "----------------------------------\n",
            "Episode Reward: 12.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "Episode Reward: 8.00 | Length: 438\n",
            "Episode Reward: 9.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 5.43     |\n",
            "|    exploration_rate | 0.17     |\n",
            "| time/               |          |\n",
            "|    episodes         | 960      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2516     |\n",
            "|    total_timesteps  | 423262   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0758   |\n",
            "|    n_updates        | 105790   |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: 8.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 5.37     |\n",
            "|    exploration_rate | 0.167    |\n",
            "| time/               |          |\n",
            "|    episodes         | 964      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2527     |\n",
            "|    total_timesteps  | 425025   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0812   |\n",
            "|    n_updates        | 106231   |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 445\n",
            "Episode Reward: 14.00 | Length: 444\n",
            "Episode Reward: 14.00 | Length: 441\n",
            "Episode Reward: 10.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.71     |\n",
            "|    exploration_rate | 0.163    |\n",
            "| time/               |          |\n",
            "|    episodes         | 968      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2538     |\n",
            "|    total_timesteps  | 426796   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0619   |\n",
            "|    n_updates        | 106673   |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "Episode Reward: 6.00 | Length: 437\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: 10.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.81     |\n",
            "|    exploration_rate | 0.16     |\n",
            "| time/               |          |\n",
            "|    episodes         | 972      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2549     |\n",
            "|    total_timesteps  | 428557   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.128    |\n",
            "|    n_updates        | 107114   |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 5.77     |\n",
            "|    exploration_rate | 0.157    |\n",
            "| time/               |          |\n",
            "|    episodes         | 976      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2560     |\n",
            "|    total_timesteps  | 430319   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0904   |\n",
            "|    n_updates        | 107554   |\n",
            "----------------------------------\n",
            "Episode Reward: 12.00 | Length: 438\n",
            "Episode Reward: 28.00 | Length: 430\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: 13.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.16     |\n",
            "|    exploration_rate | 0.153    |\n",
            "| time/               |          |\n",
            "|    episodes         | 980      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2571     |\n",
            "|    total_timesteps  | 432073   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.133    |\n",
            "|    n_updates        | 107993   |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 13.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.29     |\n",
            "|    exploration_rate | 0.15     |\n",
            "| time/               |          |\n",
            "|    episodes         | 984      |\n",
            "|    fps              | 168      |\n",
            "|    time_elapsed     | 2581     |\n",
            "|    total_timesteps  | 433839   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0923   |\n",
            "|    n_updates        | 108434   |\n",
            "----------------------------------\n",
            "Episode Reward: 15.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: 10.00 | Length: 438\n",
            "Episode Reward: -4.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.51     |\n",
            "|    exploration_rate | 0.146    |\n",
            "| time/               |          |\n",
            "|    episodes         | 988      |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2593     |\n",
            "|    total_timesteps  | 435594   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0713   |\n",
            "|    n_updates        | 108873   |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: 15.00 | Length: 439\n",
            "Episode Reward: 11.00 | Length: 439\n",
            "Episode Reward: 13.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.55     |\n",
            "|    exploration_rate | 0.143    |\n",
            "| time/               |          |\n",
            "|    episodes         | 992      |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2604     |\n",
            "|    total_timesteps  | 437354   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.121    |\n",
            "|    n_updates        | 109313   |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 437\n",
            "Episode Reward: 10.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 445\n",
            "Episode Reward: 1.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.43     |\n",
            "|    exploration_rate | 0.139    |\n",
            "| time/               |          |\n",
            "|    episodes         | 996      |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2615     |\n",
            "|    total_timesteps  | 439125   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.118    |\n",
            "|    n_updates        | 109756   |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: 5.00 | Length: 443\n",
            "Episode Reward: 12.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.2      |\n",
            "|    exploration_rate | 0.136    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1000     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2626     |\n",
            "|    total_timesteps  | 440892   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0415   |\n",
            "|    n_updates        | 110197   |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 441\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.32     |\n",
            "|    exploration_rate | 0.132    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1004     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2637     |\n",
            "|    total_timesteps  | 442653   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 110638   |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 443\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: 15.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.29     |\n",
            "|    exploration_rate | 0.129    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1008     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2649     |\n",
            "|    total_timesteps  | 444422   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.075    |\n",
            "|    n_updates        | 111080   |\n",
            "----------------------------------\n",
            "Episode Reward: 15.00 | Length: 441\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: 7.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.39     |\n",
            "|    exploration_rate | 0.125    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1012     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2660     |\n",
            "|    total_timesteps  | 446188   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0903   |\n",
            "|    n_updates        | 111521   |\n",
            "----------------------------------\n",
            "Episode Reward: 11.00 | Length: 441\n",
            "Episode Reward: 10.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.27     |\n",
            "|    exploration_rate | 0.122    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1016     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2671     |\n",
            "|    total_timesteps  | 447957   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.126    |\n",
            "|    n_updates        | 111964   |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "Episode Reward: 9.00 | Length: 445\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 6.54     |\n",
            "|    exploration_rate | 0.119    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1020     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2682     |\n",
            "|    total_timesteps  | 449724   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0578   |\n",
            "|    n_updates        | 112405   |\n",
            "----------------------------------\n",
            "Episode Reward: 17.00 | Length: 440\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: 12.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.61     |\n",
            "|    exploration_rate | 0.115    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1024     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2693     |\n",
            "|    total_timesteps  | 451493   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0878   |\n",
            "|    n_updates        | 112848   |\n",
            "----------------------------------\n",
            "Episode Reward: 19.00 | Length: 438\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.71     |\n",
            "|    exploration_rate | 0.112    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1028     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2704     |\n",
            "|    total_timesteps  | 453259   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.149    |\n",
            "|    n_updates        | 113289   |\n",
            "----------------------------------\n",
            "Episode Reward: 12.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 443\n",
            "Episode Reward: 25.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.72     |\n",
            "|    exploration_rate | 0.108    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1032     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2715     |\n",
            "|    total_timesteps  | 455028   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0943   |\n",
            "|    n_updates        | 113731   |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.57     |\n",
            "|    exploration_rate | 0.105    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1036     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2726     |\n",
            "|    total_timesteps  | 456791   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0919   |\n",
            "|    n_updates        | 114172   |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 438\n",
            "Episode Reward: 18.00 | Length: 438\n",
            "Episode Reward: 20.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.74     |\n",
            "|    exploration_rate | 0.101    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1040     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2736     |\n",
            "|    total_timesteps  | 458549   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0927   |\n",
            "|    n_updates        | 114612   |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: 9.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.55     |\n",
            "|    exploration_rate | 0.0978   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1044     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2747     |\n",
            "|    total_timesteps  | 460311   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 115052   |\n",
            "----------------------------------\n",
            "Episode Reward: 19.00 | Length: 443\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.67     |\n",
            "|    exploration_rate | 0.0943   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1048     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2758     |\n",
            "|    total_timesteps  | 462075   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0945   |\n",
            "|    n_updates        | 115493   |\n",
            "----------------------------------\n",
            "Episode Reward: 10.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.43     |\n",
            "|    exploration_rate | 0.0909   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1052     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2769     |\n",
            "|    total_timesteps  | 463836   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.134    |\n",
            "|    n_updates        | 115933   |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: 26.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.42     |\n",
            "|    exploration_rate | 0.0874   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1056     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2780     |\n",
            "|    total_timesteps  | 465601   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.07     |\n",
            "|    n_updates        | 116375   |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 442\n",
            "Episode Reward: 9.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.24     |\n",
            "|    exploration_rate | 0.084    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1060     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2791     |\n",
            "|    total_timesteps  | 467366   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.102    |\n",
            "|    n_updates        | 116816   |\n",
            "----------------------------------\n",
            "Episode Reward: 13.00 | Length: 439\n",
            "Episode Reward: 13.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.36     |\n",
            "|    exploration_rate | 0.0805   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1064     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2801     |\n",
            "|    total_timesteps  | 469131   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0927   |\n",
            "|    n_updates        | 117257   |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.06     |\n",
            "|    exploration_rate | 0.077    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1068     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2812     |\n",
            "|    total_timesteps  | 470893   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0906   |\n",
            "|    n_updates        | 117698   |\n",
            "----------------------------------\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: 12.00 | Length: 438\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.26     |\n",
            "|    exploration_rate | 0.0736   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1072     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2823     |\n",
            "|    total_timesteps  | 472659   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.101    |\n",
            "|    n_updates        | 118139   |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 11.00 | Length: 438\n",
            "Episode Reward: 13.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.37     |\n",
            "|    exploration_rate | 0.0701   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1076     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2834     |\n",
            "|    total_timesteps  | 474423   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.108    |\n",
            "|    n_updates        | 118580   |\n",
            "----------------------------------\n",
            "Episode Reward: 16.00 | Length: 440\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: 14.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.12     |\n",
            "|    exploration_rate | 0.0667   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1080     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2844     |\n",
            "|    total_timesteps  | 476180   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.354    |\n",
            "|    n_updates        | 119019   |\n",
            "----------------------------------\n",
            "Episode Reward: 14.00 | Length: 439\n",
            "Episode Reward: 10.00 | Length: 441\n",
            "Episode Reward: 8.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.28     |\n",
            "|    exploration_rate | 0.0632   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1084     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2855     |\n",
            "|    total_timesteps  | 477940   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0709   |\n",
            "|    n_updates        | 119459   |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 10.00 | Length: 443\n",
            "Episode Reward: 23.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.41     |\n",
            "|    exploration_rate | 0.0598   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1088     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2866     |\n",
            "|    total_timesteps  | 479711   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0599   |\n",
            "|    n_updates        | 119902   |\n",
            "----------------------------------\n",
            "Episode Reward: 12.00 | Length: 442\n",
            "Episode Reward: 8.00 | Length: 444\n",
            "Episode Reward: 9.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.26     |\n",
            "|    exploration_rate | 0.0563   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1092     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2877     |\n",
            "|    total_timesteps  | 481474   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0471   |\n",
            "|    n_updates        | 120343   |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: 12.00 | Length: 439\n",
            "Episode Reward: 13.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.48     |\n",
            "|    exploration_rate | 0.0529   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1096     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2888     |\n",
            "|    total_timesteps  | 483232   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.127    |\n",
            "|    n_updates        | 120782   |\n",
            "----------------------------------\n",
            "Episode Reward: 16.00 | Length: 442\n",
            "Episode Reward: 14.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.64     |\n",
            "|    exploration_rate | 0.0494   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1100     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2899     |\n",
            "|    total_timesteps  | 484997   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.136    |\n",
            "|    n_updates        | 121224   |\n",
            "----------------------------------\n",
            "Episode Reward: 16.00 | Length: 439\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "Episode Reward: 12.00 | Length: 438\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.87     |\n",
            "|    exploration_rate | 0.046    |\n",
            "| time/               |          |\n",
            "|    episodes         | 1104     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2910     |\n",
            "|    total_timesteps  | 486757   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0884   |\n",
            "|    n_updates        | 121664   |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 441\n",
            "Episode Reward: 20.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 7.04     |\n",
            "|    exploration_rate | 0.0425   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1108     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2921     |\n",
            "|    total_timesteps  | 488519   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.132    |\n",
            "|    n_updates        | 122104   |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: 15.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 6.91     |\n",
            "|    exploration_rate | 0.0391   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1112     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2931     |\n",
            "|    total_timesteps  | 490280   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.133    |\n",
            "|    n_updates        | 122544   |\n",
            "----------------------------------\n",
            "Episode Reward: 15.00 | Length: 441\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: 8.00 | Length: 443\n",
            "Episode Reward: 33.00 | Length: 432\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 7.36     |\n",
            "|    exploration_rate | 0.0356   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1116     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2942     |\n",
            "|    total_timesteps  | 492038   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.0985   |\n",
            "|    n_updates        | 122984   |\n",
            "----------------------------------\n",
            "Episode Reward: 13.00 | Length: 443\n",
            "Episode Reward: 9.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 441\n",
            "Episode Reward: 9.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | 7.55     |\n",
            "|    exploration_rate | 0.0321   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1120     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2952     |\n",
            "|    total_timesteps  | 493802   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.07     |\n",
            "|    n_updates        | 123425   |\n",
            "----------------------------------\n",
            "Episode Reward: 18.00 | Length: 439\n",
            "Episode Reward: 9.00 | Length: 444\n",
            "Episode Reward: 18.00 | Length: 438\n",
            "Episode Reward: 14.00 | Length: 366\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 7.77     |\n",
            "|    exploration_rate | 0.0288   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1124     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2962     |\n",
            "|    total_timesteps  | 495489   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.111    |\n",
            "|    n_updates        | 123847   |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "Episode Reward: 17.00 | Length: 442\n",
            "Episode Reward: 9.00 | Length: 441\n",
            "Episode Reward: 7.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 7.88     |\n",
            "|    exploration_rate | 0.0254   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1128     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2973     |\n",
            "|    total_timesteps  | 497248   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.425    |\n",
            "|    n_updates        | 124286   |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 15.00 | Length: 440\n",
            "Episode Reward: 12.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | 7.89     |\n",
            "|    exploration_rate | 0.0219   |\n",
            "| time/               |          |\n",
            "|    episodes         | 1132     |\n",
            "|    fps              | 167      |\n",
            "|    time_elapsed     | 2984     |\n",
            "|    total_timesteps  | 499007   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.0001   |\n",
            "|    loss             | 0.151    |\n",
            "|    n_updates        | 124726   |\n",
            "----------------------------------\n",
            "Episode Reward: 12.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Model saved as dqn_model.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Trying Other Hyperparameters"
      ],
      "metadata": {
        "id": "Reqv0tIBU28P"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## II. Hyperparameter Set"
      ],
      "metadata": {
        "id": "ALxRK58TVCx0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting with these parameters.\n",
        "learning_rate = 0.001\n",
        "gamma = 0.99\n",
        "batch_size = 32\n",
        "# Epsilon parameters for exploration in DQN:\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.01\n",
        "epsilon_decay = 0.98 * 1000000  # The number of timesteps over which epsilon decays"
      ],
      "metadata": {
        "id": "2c0dLAN1VXrW"
      },
      "execution_count": 40,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting with an MLP-based policy.\n",
        "policy = \"MlpPolicy\"\n",
        "\n",
        "model = DQN(\n",
        "    policy,\n",
        "    env,\n",
        "    learning_rate=learning_rate,\n",
        "    gamma=gamma,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    exploration_initial_eps=epsilon_start,\n",
        "    exploration_final_eps=epsilon_end,\n",
        "    # Adjust the exploration fraction to set decay relative to total timesteps\n",
        "    exploration_fraction=epsilon_decay / 1_000_000,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8ZWJTcncWrr5",
        "outputId": "57486efa-09c0-4d01-b1da-328036e1eb97"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Agent"
      ],
      "metadata": {
        "id": "IzDD_ZpCXBtZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for a total of 100,000 timesteps (adjust based on performance)\n",
        "total_timesteps = 100_000\n",
        "model.learn(total_timesteps=total_timesteps, callback=TrainingLogger())\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/My Drive/deep_q_learning/dqn_model2.zip\")\n",
        "print(\"Model saved as dqn_model2.zip\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mRyvsmW6XAUF",
        "outputId": "28107bd6-4e25-460e-fdd0-9b7fafe21777"
      },
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward: -13.00 | Length: 330\n",
            "Episode Reward: -11.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 443\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 415      |\n",
            "|    ep_rew_mean      | -7.75    |\n",
            "|    exploration_rate | 0.983    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 217      |\n",
            "|    time_elapsed     | 7        |\n",
            "|    total_timesteps  | 1661     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0388   |\n",
            "|    n_updates        | 390      |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "Episode Reward: -20.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 429      |\n",
            "|    ep_rew_mean      | -7.75    |\n",
            "|    exploration_rate | 0.965    |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 216      |\n",
            "|    time_elapsed     | 15       |\n",
            "|    total_timesteps  | 3432     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0427   |\n",
            "|    n_updates        | 832      |\n",
            "----------------------------------\n",
            "Episode Reward: 5.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 433      |\n",
            "|    ep_rew_mean      | -5.25    |\n",
            "|    exploration_rate | 0.948    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 24       |\n",
            "|    total_timesteps  | 5194     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.124    |\n",
            "|    n_updates        | 1273     |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "Episode Reward: -10.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 435      |\n",
            "|    ep_rew_mean      | -5.25    |\n",
            "|    exploration_rate | 0.93     |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 215      |\n",
            "|    time_elapsed     | 32       |\n",
            "|    total_timesteps  | 6960     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0623   |\n",
            "|    n_updates        | 1714     |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: -7.00 | Length: 438\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 436      |\n",
            "|    ep_rew_mean      | -4.85    |\n",
            "|    exploration_rate | 0.912    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 214      |\n",
            "|    time_elapsed     | 40       |\n",
            "|    total_timesteps  | 8717     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0501   |\n",
            "|    n_updates        | 2154     |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 437      |\n",
            "|    ep_rew_mean      | -4.33    |\n",
            "|    exploration_rate | 0.894    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 213      |\n",
            "|    time_elapsed     | 49       |\n",
            "|    total_timesteps  | 10486    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0743   |\n",
            "|    n_updates        | 2596     |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 437      |\n",
            "|    ep_rew_mean      | -4.54    |\n",
            "|    exploration_rate | 0.876    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 213      |\n",
            "|    time_elapsed     | 57       |\n",
            "|    total_timesteps  | 12247    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0398   |\n",
            "|    n_updates        | 3036     |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | -4.28    |\n",
            "|    exploration_rate | 0.859    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 213      |\n",
            "|    time_elapsed     | 65       |\n",
            "|    total_timesteps  | 14007    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0394   |\n",
            "|    n_updates        | 3476     |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | -4.19    |\n",
            "|    exploration_rate | 0.841    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 212      |\n",
            "|    time_elapsed     | 74       |\n",
            "|    total_timesteps  | 15769    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0763   |\n",
            "|    n_updates        | 3917     |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: -15.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | -4.35    |\n",
            "|    exploration_rate | 0.823    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 211      |\n",
            "|    time_elapsed     | 83       |\n",
            "|    total_timesteps  | 17531    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0467   |\n",
            "|    n_updates        | 4357     |\n",
            "----------------------------------\n",
            "Episode Reward: 6.00 | Length: 443\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.8     |\n",
            "|    exploration_rate | 0.805    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 209      |\n",
            "|    time_elapsed     | 91       |\n",
            "|    total_timesteps  | 19295    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0446   |\n",
            "|    n_updates        | 4798     |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.38    |\n",
            "|    exploration_rate | 0.787    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 208      |\n",
            "|    time_elapsed     | 100      |\n",
            "|    total_timesteps  | 21061    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0672   |\n",
            "|    n_updates        | 5240     |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 441\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: -11.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.42    |\n",
            "|    exploration_rate | 0.769    |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 208      |\n",
            "|    time_elapsed     | 109      |\n",
            "|    total_timesteps  | 22824    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0743   |\n",
            "|    n_updates        | 5680     |\n",
            "----------------------------------\n",
            "Episode Reward: -15.00 | Length: 438\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: -11.00 | Length: 441\n",
            "Episode Reward: -8.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.73    |\n",
            "|    exploration_rate | 0.752    |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 207      |\n",
            "|    time_elapsed     | 118      |\n",
            "|    total_timesteps  | 24581    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0332   |\n",
            "|    n_updates        | 6120     |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 437\n",
            "Episode Reward: -15.00 | Length: 439\n",
            "Episode Reward: -15.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.98    |\n",
            "|    exploration_rate | 0.734    |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 127      |\n",
            "|    total_timesteps  | 26335    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.104    |\n",
            "|    n_updates        | 6558     |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "Episode Reward: -12.00 | Length: 438\n",
            "Episode Reward: -4.00 | Length: 444\n",
            "Episode Reward: 6.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.91    |\n",
            "|    exploration_rate | 0.716    |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 28099    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0158   |\n",
            "|    n_updates        | 6999     |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -10.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: -9.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.97    |\n",
            "|    exploration_rate | 0.698    |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 144      |\n",
            "|    total_timesteps  | 29857    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0152   |\n",
            "|    n_updates        | 7439     |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.82    |\n",
            "|    exploration_rate | 0.681    |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 152      |\n",
            "|    total_timesteps  | 31622    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.072    |\n",
            "|    n_updates        | 7880     |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 438\n",
            "Episode Reward: -9.00 | Length: 440\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -3.83    |\n",
            "|    exploration_rate | 0.663    |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 161      |\n",
            "|    total_timesteps  | 33380    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0751   |\n",
            "|    n_updates        | 8319     |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "Episode Reward: -13.00 | Length: 439\n",
            "Episode Reward: -5.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -4.05    |\n",
            "|    exploration_rate | 0.645    |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 170      |\n",
            "|    total_timesteps  | 35143    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0794   |\n",
            "|    n_updates        | 8760     |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: -10.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -4       |\n",
            "|    exploration_rate | 0.627    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 206      |\n",
            "|    time_elapsed     | 179      |\n",
            "|    total_timesteps  | 36908    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.059    |\n",
            "|    n_updates        | 9201     |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -12.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -4.1     |\n",
            "|    exploration_rate | 0.609    |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 187      |\n",
            "|    total_timesteps  | 38669    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0322   |\n",
            "|    n_updates        | 9642     |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: -13.00 | Length: 440\n",
            "Episode Reward: -7.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -4.2     |\n",
            "|    exploration_rate | 0.592    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 196      |\n",
            "|    total_timesteps  | 40433    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0439   |\n",
            "|    n_updates        | 10083    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: -1.00 | Length: 441\n",
            "Episode Reward: -7.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.1     |\n",
            "|    exploration_rate | 0.574    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 205      |\n",
            "|    total_timesteps  | 42192    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0473   |\n",
            "|    n_updates        | 10522    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.07    |\n",
            "|    exploration_rate | 0.556    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 214      |\n",
            "|    total_timesteps  | 43970    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0455   |\n",
            "|    n_updates        | 10967    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "Episode Reward: -9.00 | Length: 442\n",
            "Episode Reward: -16.00 | Length: 444\n",
            "Episode Reward: -12.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.22    |\n",
            "|    exploration_rate | 0.538    |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 222      |\n",
            "|    total_timesteps  | 45741    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0874   |\n",
            "|    n_updates        | 11410    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 445\n",
            "Episode Reward: -5.00 | Length: 443\n",
            "Episode Reward: -8.00 | Length: 443\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.09    |\n",
            "|    exploration_rate | 0.52     |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 231      |\n",
            "|    total_timesteps  | 47514    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0315   |\n",
            "|    n_updates        | 11853    |\n",
            "----------------------------------\n",
            "Episode Reward: -27.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.51    |\n",
            "|    exploration_rate | 0.502    |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 239      |\n",
            "|    total_timesteps  | 49279    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0756   |\n",
            "|    n_updates        | 12294    |\n",
            "----------------------------------\n",
            "Episode Reward: -21.00 | Length: 437\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "Episode Reward: -16.00 | Length: 441\n",
            "Episode Reward: -6.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.79    |\n",
            "|    exploration_rate | 0.484    |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 248      |\n",
            "|    total_timesteps  | 51039    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0936   |\n",
            "|    n_updates        | 12734    |\n",
            "----------------------------------\n",
            "Episode Reward: -15.00 | Length: 439\n",
            "Episode Reward: -7.00 | Length: 445\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.99    |\n",
            "|    exploration_rate | 0.467    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 256      |\n",
            "|    total_timesteps  | 52805    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0437   |\n",
            "|    n_updates        | 13176    |\n",
            "----------------------------------\n",
            "Episode Reward: -5.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: -9.00 | Length: 440\n",
            "Episode Reward: -15.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.25    |\n",
            "|    exploration_rate | 0.449    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 265      |\n",
            "|    total_timesteps  | 54568    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0917   |\n",
            "|    n_updates        | 13616    |\n",
            "----------------------------------\n",
            "Episode Reward: -13.00 | Length: 437\n",
            "Episode Reward: -15.00 | Length: 440\n",
            "Episode Reward: -8.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.42    |\n",
            "|    exploration_rate | 0.431    |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 274      |\n",
            "|    total_timesteps  | 56325    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0809   |\n",
            "|    n_updates        | 14056    |\n",
            "----------------------------------\n",
            "Episode Reward: -13.00 | Length: 441\n",
            "Episode Reward: -12.00 | Length: 438\n",
            "Episode Reward: -9.00 | Length: 439\n",
            "Episode Reward: -12.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.78    |\n",
            "|    exploration_rate | 0.413    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 283      |\n",
            "|    total_timesteps  | 58082    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.1      |\n",
            "|    n_updates        | 14495    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -12.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 438\n",
            "Episode Reward: -15.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.98    |\n",
            "|    exploration_rate | 0.395    |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 291      |\n",
            "|    total_timesteps  | 59842    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0451   |\n",
            "|    n_updates        | 14935    |\n",
            "----------------------------------\n",
            "Episode Reward: -18.00 | Length: 442\n",
            "Episode Reward: -16.00 | Length: 442\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -6.2     |\n",
            "|    exploration_rate | 0.378    |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 300      |\n",
            "|    total_timesteps  | 61610    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0749   |\n",
            "|    n_updates        | 15377    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: -11.00 | Length: 438\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -6.5     |\n",
            "|    exploration_rate | 0.36     |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 205      |\n",
            "|    time_elapsed     | 308      |\n",
            "|    total_timesteps  | 63372    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0769   |\n",
            "|    n_updates        | 15817    |\n",
            "----------------------------------\n",
            "Episode Reward: -26.00 | Length: 438\n",
            "Episode Reward: -17.00 | Length: 442\n",
            "Episode Reward: -22.00 | Length: 444\n",
            "Episode Reward: -10.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -7.3     |\n",
            "|    exploration_rate | 0.342    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 204      |\n",
            "|    time_elapsed     | 317      |\n",
            "|    total_timesteps  | 65134    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0656   |\n",
            "|    n_updates        | 16258    |\n",
            "----------------------------------\n",
            "Episode Reward: -17.00 | Length: 443\n",
            "Episode Reward: -10.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 445\n",
            "Episode Reward: -9.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -7.57    |\n",
            "|    exploration_rate | 0.324    |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 204      |\n",
            "|    time_elapsed     | 327      |\n",
            "|    total_timesteps  | 66907    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0484   |\n",
            "|    n_updates        | 16701    |\n",
            "----------------------------------\n",
            "Episode Reward: -12.00 | Length: 440\n",
            "Episode Reward: -10.00 | Length: 442\n",
            "Episode Reward: -10.00 | Length: 438\n",
            "Episode Reward: -11.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -7.69    |\n",
            "|    exploration_rate | 0.306    |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 204      |\n",
            "|    time_elapsed     | 336      |\n",
            "|    total_timesteps  | 68670    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0949   |\n",
            "|    n_updates        | 17142    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 437\n",
            "Episode Reward: -16.00 | Length: 443\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -7.75    |\n",
            "|    exploration_rate | 0.288    |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 204      |\n",
            "|    time_elapsed     | 344      |\n",
            "|    total_timesteps  | 70434    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0635   |\n",
            "|    n_updates        | 17583    |\n",
            "----------------------------------\n",
            "Episode Reward: -14.00 | Length: 438\n",
            "Episode Reward: -18.00 | Length: 438\n",
            "Episode Reward: -14.00 | Length: 439\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -8.16    |\n",
            "|    exploration_rate | 0.271    |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 204      |\n",
            "|    time_elapsed     | 353      |\n",
            "|    total_timesteps  | 72190    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.078    |\n",
            "|    n_updates        | 18022    |\n",
            "----------------------------------\n",
            "Episode Reward: -8.00 | Length: 440\n",
            "Episode Reward: -14.00 | Length: 442\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: -17.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -8.42    |\n",
            "|    exploration_rate | 0.253    |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 203      |\n",
            "|    time_elapsed     | 362      |\n",
            "|    total_timesteps  | 73952    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0779   |\n",
            "|    n_updates        | 18462    |\n",
            "----------------------------------\n",
            "Episode Reward: -22.00 | Length: 445\n",
            "Episode Reward: -12.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 445\n",
            "Episode Reward: -11.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -8.89    |\n",
            "|    exploration_rate | 0.235    |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 203      |\n",
            "|    time_elapsed     | 371      |\n",
            "|    total_timesteps  | 75730    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0347   |\n",
            "|    n_updates        | 18907    |\n",
            "----------------------------------\n",
            "Episode Reward: -13.00 | Length: 444\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "Episode Reward: -22.00 | Length: 441\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -9.24    |\n",
            "|    exploration_rate | 0.217    |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 203      |\n",
            "|    time_elapsed     | 380      |\n",
            "|    total_timesteps  | 77497    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0455   |\n",
            "|    n_updates        | 19349    |\n",
            "----------------------------------\n",
            "Episode Reward: -19.00 | Length: 441\n",
            "Episode Reward: -23.00 | Length: 438\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -9.48    |\n",
            "|    exploration_rate | 0.199    |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 203      |\n",
            "|    time_elapsed     | 389      |\n",
            "|    total_timesteps  | 79260    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0321   |\n",
            "|    n_updates        | 19789    |\n",
            "----------------------------------\n",
            "Episode Reward: -22.00 | Length: 444\n",
            "Episode Reward: -18.00 | Length: 441\n",
            "Episode Reward: -29.00 | Length: 439\n",
            "Episode Reward: -11.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -10.2    |\n",
            "|    exploration_rate | 0.182    |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 203      |\n",
            "|    time_elapsed     | 399      |\n",
            "|    total_timesteps  | 81022    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0635   |\n",
            "|    n_updates        | 20230    |\n",
            "----------------------------------\n",
            "Episode Reward: -26.00 | Length: 439\n",
            "Episode Reward: -14.00 | Length: 444\n",
            "Episode Reward: -20.00 | Length: 440\n",
            "Episode Reward: -36.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -10.9    |\n",
            "|    exploration_rate | 0.164    |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 202      |\n",
            "|    time_elapsed     | 408      |\n",
            "|    total_timesteps  | 82787    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0585   |\n",
            "|    n_updates        | 20671    |\n",
            "----------------------------------\n",
            "Episode Reward: -35.00 | Length: 440\n",
            "Episode Reward: -15.00 | Length: 442\n",
            "Episode Reward: -16.00 | Length: 438\n",
            "Episode Reward: -24.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -11.5    |\n",
            "|    exploration_rate | 0.146    |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 202      |\n",
            "|    time_elapsed     | 417      |\n",
            "|    total_timesteps  | 84549    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.075    |\n",
            "|    n_updates        | 21112    |\n",
            "----------------------------------\n",
            "Episode Reward: -17.00 | Length: 439\n",
            "Episode Reward: -32.00 | Length: 438\n",
            "Episode Reward: -17.00 | Length: 440\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -12.2    |\n",
            "|    exploration_rate | 0.128    |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 202      |\n",
            "|    time_elapsed     | 426      |\n",
            "|    total_timesteps  | 86308    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0158   |\n",
            "|    n_updates        | 21551    |\n",
            "----------------------------------\n",
            "Episode Reward: -17.00 | Length: 441\n",
            "Episode Reward: -16.00 | Length: 437\n",
            "Episode Reward: -19.00 | Length: 437\n",
            "Episode Reward: -6.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -12.7    |\n",
            "|    exploration_rate | 0.11     |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 202      |\n",
            "|    time_elapsed     | 435      |\n",
            "|    total_timesteps  | 88061    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.091    |\n",
            "|    n_updates        | 21990    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 443\n",
            "Episode Reward: -12.00 | Length: 438\n",
            "Episode Reward: -30.00 | Length: 443\n",
            "Episode Reward: -14.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -12.9    |\n",
            "|    exploration_rate | 0.0926   |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 444      |\n",
            "|    total_timesteps  | 89827    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0312   |\n",
            "|    n_updates        | 22431    |\n",
            "----------------------------------\n",
            "Episode Reward: -31.00 | Length: 443\n",
            "Episode Reward: -31.00 | Length: 444\n",
            "Episode Reward: -18.00 | Length: 439\n",
            "Episode Reward: -22.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -13.7    |\n",
            "|    exploration_rate | 0.0747   |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 453      |\n",
            "|    total_timesteps  | 91591    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0499   |\n",
            "|    n_updates        | 22872    |\n",
            "----------------------------------\n",
            "Episode Reward: -25.00 | Length: 444\n",
            "Episode Reward: -14.00 | Length: 437\n",
            "Episode Reward: -19.00 | Length: 438\n",
            "Episode Reward: -36.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -14.2    |\n",
            "|    exploration_rate | 0.057    |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 462      |\n",
            "|    total_timesteps  | 93349    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.105    |\n",
            "|    n_updates        | 23312    |\n",
            "----------------------------------\n",
            "Episode Reward: -48.00 | Length: 445\n",
            "Episode Reward: -22.00 | Length: 443\n",
            "Episode Reward: -39.00 | Length: 440\n",
            "Episode Reward: -15.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -15      |\n",
            "|    exploration_rate | 0.0391   |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 472      |\n",
            "|    total_timesteps  | 95119    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0464   |\n",
            "|    n_updates        | 23754    |\n",
            "----------------------------------\n",
            "Episode Reward: -33.00 | Length: 438\n",
            "Episode Reward: -18.00 | Length: 444\n",
            "Episode Reward: -18.00 | Length: 440\n",
            "Episode Reward: -15.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -15.5    |\n",
            "|    exploration_rate | 0.0213   |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 481      |\n",
            "|    total_timesteps  | 96882    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0465   |\n",
            "|    n_updates        | 24195    |\n",
            "----------------------------------\n",
            "Episode Reward: -20.00 | Length: 441\n",
            "Episode Reward: -38.00 | Length: 438\n",
            "Episode Reward: -23.00 | Length: 438\n",
            "Episode Reward: -39.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -16.4    |\n",
            "|    exploration_rate | 0.01     |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 201      |\n",
            "|    time_elapsed     | 490      |\n",
            "|    total_timesteps  | 98643    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.001    |\n",
            "|    loss             | 0.0327   |\n",
            "|    n_updates        | 24635    |\n",
            "----------------------------------\n",
            "Episode Reward: -30.00 | Length: 439\n",
            "Episode Reward: -18.00 | Length: 440\n",
            "Episode Reward: -16.00 | Length: 439\n",
            "Model saved as dqn_model2.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## III. Hyperparameter Set"
      ],
      "metadata": {
        "id": "tmVn86Q0XW-1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting with these parameters.\n",
        "learning_rate = 0.002\n",
        "gamma = 0.99\n",
        "batch_size = 64\n",
        "# Epsilon parameters for exploration in DQN:\n",
        "epsilon_start = 1.0\n",
        "epsilon_end = 0.05\n",
        "epsilon_decay = 0.99 * 1000000  # The number of timesteps over which epsilon decays"
      ],
      "metadata": {
        "id": "Z_KLPlsNXW-1"
      },
      "execution_count": 43,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Experimenting with an CNN-based policy.\n",
        "policy = \"CnnPolicy\"\n",
        "\n",
        "model = DQN(\n",
        "    policy,\n",
        "    env,\n",
        "    learning_rate=learning_rate,\n",
        "    gamma=gamma,\n",
        "    batch_size=batch_size,\n",
        "    verbose=1,\n",
        "    exploration_initial_eps=epsilon_start,\n",
        "    exploration_final_eps=epsilon_end,\n",
        "    # Adjust the exploration fraction to set decay relative to total timesteps\n",
        "    exploration_fraction=epsilon_decay / 1_000_000,\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gAax0kt2XW-2",
        "outputId": "76103890-3dce-4466-8198-37b5042f9c77"
      },
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using cuda device\n",
            "Wrapping the env with a `Monitor` wrapper\n",
            "Wrapping the env in a DummyVecEnv.\n",
            "Wrapping the env in a VecTransposeImage.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Training the Agent"
      ],
      "metadata": {
        "id": "nXokPAY7XW-2"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Train for a total of 110,000 timesteps (adjust based on performance)\n",
        "total_timesteps = 110_000\n",
        "model.learn(total_timesteps=total_timesteps, callback=TrainingLogger())\n",
        "\n",
        "# Save the trained model\n",
        "model.save(\"/content/drive/My Drive/deep_q_learning/dqn_model3.zip\")\n",
        "print(\"Model saved as dqn_model3.zip\")\n",
        "env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j_lI9au2XW-2",
        "outputId": "21b7a774-e1fc-4d47-9e18-5c8a5732478f"
      },
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode Reward: 0.00 | Length: 400\n",
            "Episode Reward: 6.00 | Length: 440\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: -13.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 430      |\n",
            "|    ep_rew_mean      | -4.25    |\n",
            "|    exploration_rate | 0.985    |\n",
            "| time/               |          |\n",
            "|    episodes         | 4        |\n",
            "|    fps              | 198      |\n",
            "|    time_elapsed     | 8        |\n",
            "|    total_timesteps  | 1720     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0422   |\n",
            "|    n_updates        | 404      |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -10.00 | Length: 438\n",
            "Episode Reward: -17.00 | Length: 441\n",
            "Episode Reward: -6.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 434      |\n",
            "|    ep_rew_mean      | -6.38    |\n",
            "|    exploration_rate | 0.97     |\n",
            "| time/               |          |\n",
            "|    episodes         | 8        |\n",
            "|    fps              | 197      |\n",
            "|    time_elapsed     | 17       |\n",
            "|    total_timesteps  | 3476     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0265   |\n",
            "|    n_updates        | 843      |\n",
            "----------------------------------\n",
            "Episode Reward: -14.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 436      |\n",
            "|    ep_rew_mean      | -5.92    |\n",
            "|    exploration_rate | 0.954    |\n",
            "| time/               |          |\n",
            "|    episodes         | 12       |\n",
            "|    fps              | 196      |\n",
            "|    time_elapsed     | 26       |\n",
            "|    total_timesteps  | 5237     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0112   |\n",
            "|    n_updates        | 1284     |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -5.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | -5       |\n",
            "|    exploration_rate | 0.939    |\n",
            "| time/               |          |\n",
            "|    episodes         | 16       |\n",
            "|    fps              | 195      |\n",
            "|    time_elapsed     | 35       |\n",
            "|    total_timesteps  | 7001     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.00377  |\n",
            "|    n_updates        | 1725     |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: -13.00 | Length: 442\n",
            "Episode Reward: -5.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 438      |\n",
            "|    ep_rew_mean      | -4.95    |\n",
            "|    exploration_rate | 0.924    |\n",
            "| time/               |          |\n",
            "|    episodes         | 20       |\n",
            "|    fps              | 195      |\n",
            "|    time_elapsed     | 44       |\n",
            "|    total_timesteps  | 8765     |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0143   |\n",
            "|    n_updates        | 2166     |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 442\n",
            "Episode Reward: -11.00 | Length: 441\n",
            "Episode Reward: -11.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -5.46    |\n",
            "|    exploration_rate | 0.908    |\n",
            "| time/               |          |\n",
            "|    episodes         | 24       |\n",
            "|    fps              | 195      |\n",
            "|    time_elapsed     | 53       |\n",
            "|    total_timesteps  | 10532    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0406   |\n",
            "|    n_updates        | 2607     |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 437\n",
            "Episode Reward: -1.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -4.96    |\n",
            "|    exploration_rate | 0.893    |\n",
            "| time/               |          |\n",
            "|    episodes         | 28       |\n",
            "|    fps              | 195      |\n",
            "|    time_elapsed     | 62       |\n",
            "|    total_timesteps  | 12287    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0189   |\n",
            "|    n_updates        | 3046     |\n",
            "----------------------------------\n",
            "Episode Reward: -18.00 | Length: 438\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -5.44    |\n",
            "|    exploration_rate | 0.877    |\n",
            "| time/               |          |\n",
            "|    episodes         | 32       |\n",
            "|    fps              | 195      |\n",
            "|    time_elapsed     | 71       |\n",
            "|    total_timesteps  | 14046    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0177   |\n",
            "|    n_updates        | 3486     |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: -15.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -5.14    |\n",
            "|    exploration_rate | 0.862    |\n",
            "| time/               |          |\n",
            "|    episodes         | 36       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 81       |\n",
            "|    total_timesteps  | 15809    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.00983  |\n",
            "|    n_updates        | 3927     |\n",
            "----------------------------------\n",
            "Episode Reward: -13.00 | Length: 441\n",
            "Episode Reward: 5.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -4.92    |\n",
            "|    exploration_rate | 0.847    |\n",
            "| time/               |          |\n",
            "|    episodes         | 40       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 90       |\n",
            "|    total_timesteps  | 17572    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0313   |\n",
            "|    n_updates        | 4367     |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 437\n",
            "Episode Reward: -7.00 | Length: 445\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "Episode Reward: -10.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 439      |\n",
            "|    ep_rew_mean      | -5.11    |\n",
            "|    exploration_rate | 0.831    |\n",
            "| time/               |          |\n",
            "|    episodes         | 44       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 99       |\n",
            "|    total_timesteps  | 19336    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.025    |\n",
            "|    n_updates        | 4808     |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.73    |\n",
            "|    exploration_rate | 0.816    |\n",
            "| time/               |          |\n",
            "|    episodes         | 48       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 108      |\n",
            "|    total_timesteps  | 21099    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.047    |\n",
            "|    n_updates        | 5249     |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 445\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: -14.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.54    |\n",
            "|    exploration_rate | 0.801    |\n",
            "| time/               |          |\n",
            "|    episodes         | 52       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 117      |\n",
            "|    total_timesteps  | 22862    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0287   |\n",
            "|    n_updates        | 5690     |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 439\n",
            "Episode Reward: -17.00 | Length: 440\n",
            "Episode Reward: -4.00 | Length: 438\n",
            "Episode Reward: -6.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.82    |\n",
            "|    exploration_rate | 0.785    |\n",
            "| time/               |          |\n",
            "|    episodes         | 56       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 126      |\n",
            "|    total_timesteps  | 24617    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0291   |\n",
            "|    n_updates        | 6129     |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 438\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "Episode Reward: -3.00 | Length: 440\n",
            "Episode Reward: -17.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.8     |\n",
            "|    exploration_rate | 0.77     |\n",
            "| time/               |          |\n",
            "|    episodes         | 60       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 135      |\n",
            "|    total_timesteps  | 26378    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0282   |\n",
            "|    n_updates        | 6569     |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 444\n",
            "Episode Reward: -12.00 | Length: 442\n",
            "Episode Reward: -19.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -5.06    |\n",
            "|    exploration_rate | 0.754    |\n",
            "| time/               |          |\n",
            "|    episodes         | 64       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 144      |\n",
            "|    total_timesteps  | 28151    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0331   |\n",
            "|    n_updates        | 7012     |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -1.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.78    |\n",
            "|    exploration_rate | 0.739    |\n",
            "| time/               |          |\n",
            "|    episodes         | 68       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 153      |\n",
            "|    total_timesteps  | 29916    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0312   |\n",
            "|    n_updates        | 7453     |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "Episode Reward: 6.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.5     |\n",
            "|    exploration_rate | 0.724    |\n",
            "| time/               |          |\n",
            "|    episodes         | 72       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 162      |\n",
            "|    total_timesteps  | 31676    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0359   |\n",
            "|    n_updates        | 7893     |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 445\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.41    |\n",
            "|    exploration_rate | 0.708    |\n",
            "| time/               |          |\n",
            "|    episodes         | 76       |\n",
            "|    fps              | 194      |\n",
            "|    time_elapsed     | 172      |\n",
            "|    total_timesteps  | 33444    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0273   |\n",
            "|    n_updates        | 8335     |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: -10.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.35    |\n",
            "|    exploration_rate | 0.693    |\n",
            "| time/               |          |\n",
            "|    episodes         | 80       |\n",
            "|    fps              | 193      |\n",
            "|    time_elapsed     | 181      |\n",
            "|    total_timesteps  | 35206    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.018    |\n",
            "|    n_updates        | 8776     |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 438\n",
            "Episode Reward: -8.00 | Length: 440\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.39    |\n",
            "|    exploration_rate | 0.678    |\n",
            "| time/               |          |\n",
            "|    episodes         | 84       |\n",
            "|    fps              | 193      |\n",
            "|    time_elapsed     | 190      |\n",
            "|    total_timesteps  | 36963    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.022    |\n",
            "|    n_updates        | 9215     |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.34    |\n",
            "|    exploration_rate | 0.662    |\n",
            "| time/               |          |\n",
            "|    episodes         | 88       |\n",
            "|    fps              | 193      |\n",
            "|    time_elapsed     | 199      |\n",
            "|    total_timesteps  | 38732    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0364   |\n",
            "|    n_updates        | 9657     |\n",
            "----------------------------------\n",
            "Episode Reward: -8.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "Episode Reward: -4.00 | Length: 439\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.35    |\n",
            "|    exploration_rate | 0.647    |\n",
            "| time/               |          |\n",
            "|    episodes         | 92       |\n",
            "|    fps              | 193      |\n",
            "|    time_elapsed     | 209      |\n",
            "|    total_timesteps  | 40494    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0434   |\n",
            "|    n_updates        | 10098    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.39    |\n",
            "|    exploration_rate | 0.631    |\n",
            "| time/               |          |\n",
            "|    episodes         | 96       |\n",
            "|    fps              | 193      |\n",
            "|    time_elapsed     | 218      |\n",
            "|    total_timesteps  | 42265    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0359   |\n",
            "|    n_updates        | 10541    |\n",
            "----------------------------------\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "Episode Reward: 8.00 | Length: 439\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 440      |\n",
            "|    ep_rew_mean      | -4.18    |\n",
            "|    exploration_rate | 0.616    |\n",
            "| time/               |          |\n",
            "|    episodes         | 100      |\n",
            "|    fps              | 192      |\n",
            "|    time_elapsed     | 228      |\n",
            "|    total_timesteps  | 44029    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0238   |\n",
            "|    n_updates        | 10982    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: 5.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.97    |\n",
            "|    exploration_rate | 0.601    |\n",
            "| time/               |          |\n",
            "|    episodes         | 104      |\n",
            "|    fps              | 192      |\n",
            "|    time_elapsed     | 237      |\n",
            "|    total_timesteps  | 45791    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0419   |\n",
            "|    n_updates        | 11422    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: -1.00 | Length: 445\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.72    |\n",
            "|    exploration_rate | 0.585    |\n",
            "| time/               |          |\n",
            "|    episodes         | 108      |\n",
            "|    fps              | 192      |\n",
            "|    time_elapsed     | 247      |\n",
            "|    total_timesteps  | 47568    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0369   |\n",
            "|    n_updates        | 11866    |\n",
            "----------------------------------\n",
            "Episode Reward: -12.00 | Length: 444\n",
            "Episode Reward: -8.00 | Length: 439\n",
            "Episode Reward: -13.00 | Length: 441\n",
            "Episode Reward: -4.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.89    |\n",
            "|    exploration_rate | 0.57     |\n",
            "| time/               |          |\n",
            "|    episodes         | 112      |\n",
            "|    fps              | 192      |\n",
            "|    time_elapsed     | 256      |\n",
            "|    total_timesteps  | 49335    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0512   |\n",
            "|    n_updates        | 12308    |\n",
            "----------------------------------\n",
            "Episode Reward: -16.00 | Length: 443\n",
            "Episode Reward: -2.00 | Length: 438\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "Episode Reward: -20.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.24    |\n",
            "|    exploration_rate | 0.554    |\n",
            "| time/               |          |\n",
            "|    episodes         | 116      |\n",
            "|    fps              | 192      |\n",
            "|    time_elapsed     | 265      |\n",
            "|    total_timesteps  | 51103    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0475   |\n",
            "|    n_updates        | 12750    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 442\n",
            "Episode Reward: -10.00 | Length: 440\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.31    |\n",
            "|    exploration_rate | 0.539    |\n",
            "| time/               |          |\n",
            "|    episodes         | 120      |\n",
            "|    fps              | 191      |\n",
            "|    time_elapsed     | 275      |\n",
            "|    total_timesteps  | 52865    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0369   |\n",
            "|    n_updates        | 13191    |\n",
            "----------------------------------\n",
            "Episode Reward: -13.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 440\n",
            "Episode Reward: -14.00 | Length: 443\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.37    |\n",
            "|    exploration_rate | 0.523    |\n",
            "| time/               |          |\n",
            "|    episodes         | 124      |\n",
            "|    fps              | 191      |\n",
            "|    time_elapsed     | 284      |\n",
            "|    total_timesteps  | 54631    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.063    |\n",
            "|    n_updates        | 13632    |\n",
            "----------------------------------\n",
            "Episode Reward: -22.00 | Length: 444\n",
            "Episode Reward: 2.00 | Length: 439\n",
            "Episode Reward: 2.00 | Length: 441\n",
            "Episode Reward: -9.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.56    |\n",
            "|    exploration_rate | 0.508    |\n",
            "| time/               |          |\n",
            "|    episodes         | 128      |\n",
            "|    fps              | 191      |\n",
            "|    time_elapsed     | 294      |\n",
            "|    total_timesteps  | 56396    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0399   |\n",
            "|    n_updates        | 14073    |\n",
            "----------------------------------\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 445\n",
            "Episode Reward: -1.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.36    |\n",
            "|    exploration_rate | 0.493    |\n",
            "| time/               |          |\n",
            "|    episodes         | 132      |\n",
            "|    fps              | 191      |\n",
            "|    time_elapsed     | 304      |\n",
            "|    total_timesteps  | 58167    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0372   |\n",
            "|    n_updates        | 14516    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: -11.00 | Length: 440\n",
            "Episode Reward: -11.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.43    |\n",
            "|    exploration_rate | 0.477    |\n",
            "| time/               |          |\n",
            "|    episodes         | 136      |\n",
            "|    fps              | 191      |\n",
            "|    time_elapsed     | 313      |\n",
            "|    total_timesteps  | 59933    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0266   |\n",
            "|    n_updates        | 14958    |\n",
            "----------------------------------\n",
            "Episode Reward: -20.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.63    |\n",
            "|    exploration_rate | 0.462    |\n",
            "| time/               |          |\n",
            "|    episodes         | 140      |\n",
            "|    fps              | 190      |\n",
            "|    time_elapsed     | 323      |\n",
            "|    total_timesteps  | 61705    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0354   |\n",
            "|    n_updates        | 15401    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 438\n",
            "Episode Reward: -3.00 | Length: 444\n",
            "Episode Reward: 3.00 | Length: 438\n",
            "Episode Reward: -9.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.42    |\n",
            "|    exploration_rate | 0.446    |\n",
            "| time/               |          |\n",
            "|    episodes         | 144      |\n",
            "|    fps              | 190      |\n",
            "|    time_elapsed     | 332      |\n",
            "|    total_timesteps  | 63467    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.048    |\n",
            "|    n_updates        | 15841    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 438\n",
            "Episode Reward: -13.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.73    |\n",
            "|    exploration_rate | 0.431    |\n",
            "| time/               |          |\n",
            "|    episodes         | 148      |\n",
            "|    fps              | 190      |\n",
            "|    time_elapsed     | 342      |\n",
            "|    total_timesteps  | 65230    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.023    |\n",
            "|    n_updates        | 16282    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 442\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "Episode Reward: -3.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.8     |\n",
            "|    exploration_rate | 0.416    |\n",
            "| time/               |          |\n",
            "|    episodes         | 152      |\n",
            "|    fps              | 190      |\n",
            "|    time_elapsed     | 352      |\n",
            "|    total_timesteps  | 66996    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0307   |\n",
            "|    n_updates        | 16723    |\n",
            "----------------------------------\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: -4.00 | Length: 440\n",
            "Episode Reward: -9.00 | Length: 443\n",
            "Episode Reward: -8.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.64    |\n",
            "|    exploration_rate | 0.4      |\n",
            "| time/               |          |\n",
            "|    episodes         | 156      |\n",
            "|    fps              | 189      |\n",
            "|    time_elapsed     | 361      |\n",
            "|    total_timesteps  | 68763    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0289   |\n",
            "|    n_updates        | 17165    |\n",
            "----------------------------------\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "Episode Reward: -12.00 | Length: 441\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.75    |\n",
            "|    exploration_rate | 0.385    |\n",
            "| time/               |          |\n",
            "|    episodes         | 160      |\n",
            "|    fps              | 189      |\n",
            "|    time_elapsed     | 371      |\n",
            "|    total_timesteps  | 70526    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0626   |\n",
            "|    n_updates        | 17606    |\n",
            "----------------------------------\n",
            "Episode Reward: -10.00 | Length: 439\n",
            "Episode Reward: -4.00 | Length: 437\n",
            "Episode Reward: -18.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.74    |\n",
            "|    exploration_rate | 0.369    |\n",
            "| time/               |          |\n",
            "|    episodes         | 164      |\n",
            "|    fps              | 189      |\n",
            "|    time_elapsed     | 381      |\n",
            "|    total_timesteps  | 72281    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0532   |\n",
            "|    n_updates        | 18045    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 438\n",
            "Episode Reward: -7.00 | Length: 439\n",
            "Episode Reward: -2.00 | Length: 443\n",
            "Episode Reward: -14.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.95    |\n",
            "|    exploration_rate | 0.354    |\n",
            "| time/               |          |\n",
            "|    episodes         | 168      |\n",
            "|    fps              | 189      |\n",
            "|    time_elapsed     | 391      |\n",
            "|    total_timesteps  | 74042    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0584   |\n",
            "|    n_updates        | 18485    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: -5.00 | Length: 444\n",
            "Episode Reward: -5.00 | Length: 440\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.2     |\n",
            "|    exploration_rate | 0.339    |\n",
            "| time/               |          |\n",
            "|    episodes         | 172      |\n",
            "|    fps              | 189      |\n",
            "|    time_elapsed     | 400      |\n",
            "|    total_timesteps  | 75810    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0531   |\n",
            "|    n_updates        | 18927    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 3.00 | Length: 440\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.11    |\n",
            "|    exploration_rate | 0.323    |\n",
            "| time/               |          |\n",
            "|    episodes         | 176      |\n",
            "|    fps              | 188      |\n",
            "|    time_elapsed     | 410      |\n",
            "|    total_timesteps  | 77575    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0551   |\n",
            "|    n_updates        | 19368    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 444\n",
            "Episode Reward: -7.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 444\n",
            "Episode Reward: -2.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -5.04    |\n",
            "|    exploration_rate | 0.308    |\n",
            "| time/               |          |\n",
            "|    episodes         | 180      |\n",
            "|    fps              | 188      |\n",
            "|    time_elapsed     | 420      |\n",
            "|    total_timesteps  | 79342    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0499   |\n",
            "|    n_updates        | 19810    |\n",
            "----------------------------------\n",
            "Episode Reward: -2.00 | Length: 444\n",
            "Episode Reward: 4.00 | Length: 441\n",
            "Episode Reward: 7.00 | Length: 444\n",
            "Episode Reward: 5.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.69    |\n",
            "|    exploration_rate | 0.292    |\n",
            "| time/               |          |\n",
            "|    episodes         | 184      |\n",
            "|    fps              | 187      |\n",
            "|    time_elapsed     | 431      |\n",
            "|    total_timesteps  | 81108    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0594   |\n",
            "|    n_updates        | 20251    |\n",
            "----------------------------------\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: -5.00 | Length: 445\n",
            "Episode Reward: -11.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.69    |\n",
            "|    exploration_rate | 0.277    |\n",
            "| time/               |          |\n",
            "|    episodes         | 188      |\n",
            "|    fps              | 187      |\n",
            "|    time_elapsed     | 441      |\n",
            "|    total_timesteps  | 82879    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0304   |\n",
            "|    n_updates        | 20694    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 439\n",
            "Episode Reward: 5.00 | Length: 441\n",
            "Episode Reward: -3.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.52    |\n",
            "|    exploration_rate | 0.262    |\n",
            "| time/               |          |\n",
            "|    episodes         | 192      |\n",
            "|    fps              | 187      |\n",
            "|    time_elapsed     | 452      |\n",
            "|    total_timesteps  | 84641    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0596   |\n",
            "|    n_updates        | 21135    |\n",
            "----------------------------------\n",
            "Episode Reward: -1.00 | Length: 438\n",
            "Episode Reward: -9.00 | Length: 437\n",
            "Episode Reward: 6.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 437\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.41    |\n",
            "|    exploration_rate | 0.246    |\n",
            "| time/               |          |\n",
            "|    episodes         | 196      |\n",
            "|    fps              | 186      |\n",
            "|    time_elapsed     | 462      |\n",
            "|    total_timesteps  | 86396    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0281   |\n",
            "|    n_updates        | 21573    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 441\n",
            "Episode Reward: -4.00 | Length: 444\n",
            "Episode Reward: -4.00 | Length: 439\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.64    |\n",
            "|    exploration_rate | 0.231    |\n",
            "| time/               |          |\n",
            "|    episodes         | 200      |\n",
            "|    fps              | 186      |\n",
            "|    time_elapsed     | 473      |\n",
            "|    total_timesteps  | 88162    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0539   |\n",
            "|    n_updates        | 22015    |\n",
            "----------------------------------\n",
            "Episode Reward: -3.00 | Length: 441\n",
            "Episode Reward: 1.00 | Length: 439\n",
            "Episode Reward: -5.00 | Length: 439\n",
            "Episode Reward: 4.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.71    |\n",
            "|    exploration_rate | 0.216    |\n",
            "| time/               |          |\n",
            "|    episodes         | 204      |\n",
            "|    fps              | 186      |\n",
            "|    time_elapsed     | 483      |\n",
            "|    total_timesteps  | 89920    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0282   |\n",
            "|    n_updates        | 22454    |\n",
            "----------------------------------\n",
            "Episode Reward: -9.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 443\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "Episode Reward: -4.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.78    |\n",
            "|    exploration_rate | 0.2      |\n",
            "| time/               |          |\n",
            "|    episodes         | 208      |\n",
            "|    fps              | 185      |\n",
            "|    time_elapsed     | 493      |\n",
            "|    total_timesteps  | 91691    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0628   |\n",
            "|    n_updates        | 22897    |\n",
            "----------------------------------\n",
            "Episode Reward: -19.00 | Length: 438\n",
            "Episode Reward: 8.00 | Length: 438\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -6.00 | Length: 441\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.58    |\n",
            "|    exploration_rate | 0.185    |\n",
            "| time/               |          |\n",
            "|    episodes         | 212      |\n",
            "|    fps              | 185      |\n",
            "|    time_elapsed     | 503      |\n",
            "|    total_timesteps  | 93446    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0497   |\n",
            "|    n_updates        | 23336    |\n",
            "----------------------------------\n",
            "Episode Reward: 4.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 439\n",
            "Episode Reward: 1.00 | Length: 440\n",
            "Episode Reward: 3.00 | Length: 444\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.01    |\n",
            "|    exploration_rate | 0.169    |\n",
            "| time/               |          |\n",
            "|    episodes         | 216      |\n",
            "|    fps              | 185      |\n",
            "|    time_elapsed     | 514      |\n",
            "|    total_timesteps  | 95211    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0906   |\n",
            "|    n_updates        | 23777    |\n",
            "----------------------------------\n",
            "Episode Reward: -4.00 | Length: 443\n",
            "Episode Reward: -1.00 | Length: 440\n",
            "Episode Reward: -6.00 | Length: 444\n",
            "Episode Reward: -18.00 | Length: 439\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -4.04    |\n",
            "|    exploration_rate | 0.154    |\n",
            "| time/               |          |\n",
            "|    episodes         | 220      |\n",
            "|    fps              | 184      |\n",
            "|    time_elapsed     | 524      |\n",
            "|    total_timesteps  | 96977    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0337   |\n",
            "|    n_updates        | 24219    |\n",
            "----------------------------------\n",
            "Episode Reward: 0.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 438\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.68    |\n",
            "|    exploration_rate | 0.139    |\n",
            "| time/               |          |\n",
            "|    episodes         | 224      |\n",
            "|    fps              | 184      |\n",
            "|    time_elapsed     | 535      |\n",
            "|    total_timesteps  | 98741    |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0458   |\n",
            "|    n_updates        | 24660    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 441\n",
            "Episode Reward: 0.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.51    |\n",
            "|    exploration_rate | 0.123    |\n",
            "| time/               |          |\n",
            "|    episodes         | 228      |\n",
            "|    fps              | 184      |\n",
            "|    time_elapsed     | 545      |\n",
            "|    total_timesteps  | 100505   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0348   |\n",
            "|    n_updates        | 25101    |\n",
            "----------------------------------\n",
            "Episode Reward: 1.00 | Length: 443\n",
            "Episode Reward: 0.00 | Length: 444\n",
            "Episode Reward: -14.00 | Length: 439\n",
            "Episode Reward: -3.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.52    |\n",
            "|    exploration_rate | 0.108    |\n",
            "| time/               |          |\n",
            "|    episodes         | 232      |\n",
            "|    fps              | 183      |\n",
            "|    time_elapsed     | 556      |\n",
            "|    total_timesteps  | 102269   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0491   |\n",
            "|    n_updates        | 25542    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 443\n",
            "Episode Reward: 2.00 | Length: 443\n",
            "Episode Reward: -8.00 | Length: 442\n",
            "Episode Reward: -4.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.51    |\n",
            "|    exploration_rate | 0.0924   |\n",
            "| time/               |          |\n",
            "|    episodes         | 236      |\n",
            "|    fps              | 183      |\n",
            "|    time_elapsed     | 566      |\n",
            "|    total_timesteps  | 104039   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0482   |\n",
            "|    n_updates        | 25984    |\n",
            "----------------------------------\n",
            "Episode Reward: -11.00 | Length: 443\n",
            "Episode Reward: -6.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: 8.00 | Length: 438\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.21    |\n",
            "|    exploration_rate | 0.077    |\n",
            "| time/               |          |\n",
            "|    episodes         | 240      |\n",
            "|    fps              | 183      |\n",
            "|    time_elapsed     | 576      |\n",
            "|    total_timesteps  | 105804   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0828   |\n",
            "|    n_updates        | 26425    |\n",
            "----------------------------------\n",
            "Episode Reward: -12.00 | Length: 440\n",
            "Episode Reward: 1.00 | Length: 442\n",
            "Episode Reward: 5.00 | Length: 440\n",
            "Episode Reward: 2.00 | Length: 442\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -3.18    |\n",
            "|    exploration_rate | 0.0616   |\n",
            "| time/               |          |\n",
            "|    episodes         | 244      |\n",
            "|    fps              | 183      |\n",
            "|    time_elapsed     | 587      |\n",
            "|    total_timesteps  | 107568   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0783   |\n",
            "|    n_updates        | 26866    |\n",
            "----------------------------------\n",
            "Episode Reward: 7.00 | Length: 442\n",
            "Episode Reward: 7.00 | Length: 440\n",
            "Episode Reward: 0.00 | Length: 437\n",
            "Episode Reward: 2.00 | Length: 445\n",
            "----------------------------------\n",
            "| rollout/            |          |\n",
            "|    ep_len_mean      | 441      |\n",
            "|    ep_rew_mean      | -2.69    |\n",
            "|    exploration_rate | 0.05     |\n",
            "| time/               |          |\n",
            "|    episodes         | 248      |\n",
            "|    fps              | 182      |\n",
            "|    time_elapsed     | 597      |\n",
            "|    total_timesteps  | 109332   |\n",
            "| train/              |          |\n",
            "|    learning_rate    | 0.002    |\n",
            "|    loss             | 0.0363   |\n",
            "|    n_updates        | 27307    |\n",
            "----------------------------------\n",
            "Episode Reward: -7.00 | Length: 444\n",
            "Model saved as dqn_model3.zip\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "x2crO-TyUb25"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y libsdl2-dev libsdl2-image-dev libsdl2-mixer-dev libsdl2-ttf-dev\n",
        "!pip install pyvirtualdisplay"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lfCtmg5GV5_0",
        "outputId": "a3a6f1f2-b203-4615-c691-b1079e330e60"
      },
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  gir1.2-ibus-1.0 libdbus-1-dev libdecor-0-dev libdrm-dev libegl-dev libegl1-mesa-dev\n",
            "  libfluidsynth3 libgbm-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev\n",
            "  libglvnd-core-dev libglvnd-dev libglx-dev libibus-1.0-5 libibus-1.0-dev libinstpatch-1.0-2\n",
            "  libmodplug1 libopengl-dev libopusfile0 libpciaccess-dev libpulse-dev libpulse-mainloop-glib0\n",
            "  libsdl2-image-2.0-0 libsdl2-mixer-2.0-0 libsdl2-ttf-2.0-0 libsndio-dev libudev-dev libwayland-bin\n",
            "  libwayland-dev libxcursor-dev libxfixes-dev libxi-dev libxinerama-dev libxkbcommon-dev\n",
            "  libxrandr-dev libxt-dev libxv-dev libxxf86vm-dev timgm6mb-soundfont\n",
            "Suggested packages:\n",
            "  libwayland-doc libxt-doc fluid-soundfont-gm\n",
            "The following NEW packages will be installed:\n",
            "  gir1.2-ibus-1.0 libdbus-1-dev libdecor-0-dev libdrm-dev libegl-dev libegl1-mesa-dev\n",
            "  libfluidsynth3 libgbm-dev libgl-dev libgles-dev libgles1 libglu1-mesa libglu1-mesa-dev\n",
            "  libglvnd-core-dev libglvnd-dev libglx-dev libibus-1.0-5 libibus-1.0-dev libinstpatch-1.0-2\n",
            "  libmodplug1 libopengl-dev libopusfile0 libpciaccess-dev libpulse-dev libpulse-mainloop-glib0\n",
            "  libsdl2-dev libsdl2-image-2.0-0 libsdl2-image-dev libsdl2-mixer-2.0-0 libsdl2-mixer-dev\n",
            "  libsdl2-ttf-2.0-0 libsdl2-ttf-dev libsndio-dev libudev-dev libwayland-bin libwayland-dev\n",
            "  libxcursor-dev libxfixes-dev libxi-dev libxinerama-dev libxkbcommon-dev libxrandr-dev libxt-dev\n",
            "  libxv-dev libxxf86vm-dev timgm6mb-soundfont\n",
            "0 upgraded, 46 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 10.8 MB of archives.\n",
            "After this operation, 33.9 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibus-1.0-5 amd64 1.5.26-4 [183 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 gir1.2-ibus-1.0 amd64 1.5.26-4 [88.3 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdbus-1-dev amd64 1.12.20-2ubuntu4.1 [188 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-bin amd64 1.20.0-1ubuntu0.1 [20.4 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libwayland-dev amd64 1.20.0-1ubuntu0.1 [69.5 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 libdecor-0-dev amd64 0.1.0-3build1 [5,544 B]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 libpciaccess-dev amd64 0.16-3 [21.9 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libdrm-dev amd64 2.4.113-2~ubuntu0.22.04.1 [292 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglx-dev amd64 1.4.0-1 [14.1 kB]\n",
            "Get:10 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgl-dev amd64 1.4.0-1 [101 kB]\n",
            "Get:11 http://archive.ubuntu.com/ubuntu jammy/main amd64 libegl-dev amd64 1.4.0-1 [18.0 kB]\n",
            "Get:12 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-core-dev amd64 1.4.0-1 [12.7 kB]\n",
            "Get:13 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles1 amd64 1.4.0-1 [11.5 kB]\n",
            "Get:14 http://archive.ubuntu.com/ubuntu jammy/main amd64 libgles-dev amd64 1.4.0-1 [49.4 kB]\n",
            "Get:15 http://archive.ubuntu.com/ubuntu jammy/main amd64 libopengl-dev amd64 1.4.0-1 [3,400 B]\n",
            "Get:16 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglvnd-dev amd64 1.4.0-1 [3,162 B]\n",
            "Get:17 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libegl1-mesa-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [11.1 kB]\n",
            "Get:18 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libinstpatch-1.0-2 amd64 1.1.6-1 [240 kB]\n",
            "Get:19 http://archive.ubuntu.com/ubuntu jammy/universe amd64 timgm6mb-soundfont all 1.3-5 [5,427 kB]\n",
            "Get:20 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libfluidsynth3 amd64 2.2.5-1 [246 kB]\n",
            "Get:21 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libgbm-dev amd64 23.2.1-1ubuntu3.1~22.04.3 [9,542 B]\n",
            "Get:22 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa amd64 9.0.2-1 [145 kB]\n",
            "Get:23 http://archive.ubuntu.com/ubuntu jammy/main amd64 libglu1-mesa-dev amd64 9.0.2-1 [231 kB]\n",
            "Get:24 http://archive.ubuntu.com/ubuntu jammy/main amd64 libibus-1.0-dev amd64 1.5.26-4 [185 kB]\n",
            "Get:25 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libmodplug1 amd64 1:0.8.9.0-3 [153 kB]\n",
            "Get:26 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libopusfile0 amd64 0.9+20170913-1.1build1 [43.2 kB]\n",
            "Get:27 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse-mainloop-glib0 amd64 1:15.99.1+dfsg1-1ubuntu2.2 [12.4 kB]\n",
            "Get:28 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libpulse-dev amd64 1:15.99.1+dfsg1-1ubuntu2.2 [75.6 kB]\n",
            "Get:29 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsndio-dev amd64 1.8.1-1.1 [17.8 kB]\n",
            "Get:30 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 libudev-dev amd64 249.11-0ubuntu3.12 [20.7 kB]\n",
            "Get:31 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfixes-dev amd64 1:6.0.0-1 [12.2 kB]\n",
            "Get:32 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxcursor-dev amd64 1:1.2.0-2build4 [28.2 kB]\n",
            "Get:33 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxi-dev amd64 2:1.8-1build1 [193 kB]\n",
            "Get:34 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxinerama-dev amd64 2:1.1.4-3 [8,104 B]\n",
            "Get:35 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbcommon-dev amd64 1.4.0-1 [54.9 kB]\n",
            "Get:36 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxrandr-dev amd64 2:1.5.2-1build1 [26.7 kB]\n",
            "Get:37 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxt-dev amd64 1:1.2.1-1 [396 kB]\n",
            "Get:38 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxv-dev amd64 2:1.0.11-1build2 [33.4 kB]\n",
            "Get:39 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxxf86vm-dev amd64 1:1.1.4-1build3 [13.9 kB]\n",
            "Get:40 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 libsdl2-dev amd64 2.0.20+dfsg-2ubuntu1.22.04.1 [1,767 kB]\n",
            "Get:41 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsdl2-image-2.0-0 amd64 2.0.5+dfsg1-3build1 [70.4 kB]\n",
            "Get:42 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsdl2-image-dev amd64 2.0.5+dfsg1-3build1 [76.9 kB]\n",
            "Get:43 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsdl2-mixer-2.0-0 amd64 2.0.4+dfsg1-4build1 [65.9 kB]\n",
            "Get:44 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsdl2-mixer-dev amd64 2.0.4+dfsg1-4build1 [83.3 kB]\n",
            "Get:45 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsdl2-ttf-2.0-0 amd64 2.0.18+dfsg-2 [30.9 kB]\n",
            "Get:46 http://archive.ubuntu.com/ubuntu jammy/universe amd64 libsdl2-ttf-dev amd64 2.0.18+dfsg-2 [35.9 kB]\n",
            "Fetched 10.8 MB in 1s (19.0 MB/s)\n",
            "Extracting templates from packages: 100%\n",
            "Selecting previously unselected package libibus-1.0-5:amd64.\n",
            "(Reading database ... 126209 files and directories currently installed.)\n",
            "Preparing to unpack .../00-libibus-1.0-5_1.5.26-4_amd64.deb ...\n",
            "Unpacking libibus-1.0-5:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package gir1.2-ibus-1.0:amd64.\n",
            "Preparing to unpack .../01-gir1.2-ibus-1.0_1.5.26-4_amd64.deb ...\n",
            "Unpacking gir1.2-ibus-1.0:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package libdbus-1-dev:amd64.\n",
            "Preparing to unpack .../02-libdbus-1-dev_1.12.20-2ubuntu4.1_amd64.deb ...\n",
            "Unpacking libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Selecting previously unselected package libwayland-bin.\n",
            "Preparing to unpack .../03-libwayland-bin_1.20.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libwayland-bin (1.20.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libwayland-dev:amd64.\n",
            "Preparing to unpack .../04-libwayland-dev_1.20.0-1ubuntu0.1_amd64.deb ...\n",
            "Unpacking libwayland-dev:amd64 (1.20.0-1ubuntu0.1) ...\n",
            "Selecting previously unselected package libdecor-0-dev:amd64.\n",
            "Preparing to unpack .../05-libdecor-0-dev_0.1.0-3build1_amd64.deb ...\n",
            "Unpacking libdecor-0-dev:amd64 (0.1.0-3build1) ...\n",
            "Selecting previously unselected package libpciaccess-dev:amd64.\n",
            "Preparing to unpack .../06-libpciaccess-dev_0.16-3_amd64.deb ...\n",
            "Unpacking libpciaccess-dev:amd64 (0.16-3) ...\n",
            "Selecting previously unselected package libdrm-dev:amd64.\n",
            "Preparing to unpack .../07-libdrm-dev_2.4.113-2~ubuntu0.22.04.1_amd64.deb ...\n",
            "Unpacking libdrm-dev:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
            "Selecting previously unselected package libglx-dev:amd64.\n",
            "Preparing to unpack .../08-libglx-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglx-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgl-dev:amd64.\n",
            "Preparing to unpack .../09-libgl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl-dev:amd64.\n",
            "Preparing to unpack .../10-libegl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libegl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-core-dev:amd64.\n",
            "Preparing to unpack .../11-libglvnd-core-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles1:amd64.\n",
            "Preparing to unpack .../12-libgles1_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles1:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libgles-dev:amd64.\n",
            "Preparing to unpack .../13-libgles-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libgles-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libopengl-dev:amd64.\n",
            "Preparing to unpack .../14-libopengl-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libglvnd-dev:amd64.\n",
            "Preparing to unpack .../15-libglvnd-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libegl1-mesa-dev:amd64.\n",
            "Preparing to unpack .../16-libegl1-mesa-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libinstpatch-1.0-2:amd64.\n",
            "Preparing to unpack .../17-libinstpatch-1.0-2_1.1.6-1_amd64.deb ...\n",
            "Unpacking libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Selecting previously unselected package timgm6mb-soundfont.\n",
            "Preparing to unpack .../18-timgm6mb-soundfont_1.3-5_all.deb ...\n",
            "Unpacking timgm6mb-soundfont (1.3-5) ...\n",
            "Selecting previously unselected package libfluidsynth3:amd64.\n",
            "Preparing to unpack .../19-libfluidsynth3_2.2.5-1_amd64.deb ...\n",
            "Unpacking libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Selecting previously unselected package libgbm-dev:amd64.\n",
            "Preparing to unpack .../20-libgbm-dev_23.2.1-1ubuntu3.1~22.04.3_amd64.deb ...\n",
            "Unpacking libgbm-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Selecting previously unselected package libglu1-mesa:amd64.\n",
            "Preparing to unpack .../21-libglu1-mesa_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libglu1-mesa-dev:amd64.\n",
            "Preparing to unpack .../22-libglu1-mesa-dev_9.0.2-1_amd64.deb ...\n",
            "Unpacking libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Selecting previously unselected package libibus-1.0-dev:amd64.\n",
            "Preparing to unpack .../23-libibus-1.0-dev_1.5.26-4_amd64.deb ...\n",
            "Unpacking libibus-1.0-dev:amd64 (1.5.26-4) ...\n",
            "Selecting previously unselected package libmodplug1:amd64.\n",
            "Preparing to unpack .../24-libmodplug1_1%3a0.8.9.0-3_amd64.deb ...\n",
            "Unpacking libmodplug1:amd64 (1:0.8.9.0-3) ...\n",
            "Selecting previously unselected package libopusfile0.\n",
            "Preparing to unpack .../25-libopusfile0_0.9+20170913-1.1build1_amd64.deb ...\n",
            "Unpacking libopusfile0 (0.9+20170913-1.1build1) ...\n",
            "Selecting previously unselected package libpulse-mainloop-glib0:amd64.\n",
            "Preparing to unpack .../26-libpulse-mainloop-glib0_1%3a15.99.1+dfsg1-1ubuntu2.2_amd64.deb ...\n",
            "Unpacking libpulse-mainloop-glib0:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Selecting previously unselected package libpulse-dev:amd64.\n",
            "Preparing to unpack .../27-libpulse-dev_1%3a15.99.1+dfsg1-1ubuntu2.2_amd64.deb ...\n",
            "Unpacking libpulse-dev:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Selecting previously unselected package libsndio-dev:amd64.\n",
            "Preparing to unpack .../28-libsndio-dev_1.8.1-1.1_amd64.deb ...\n",
            "Unpacking libsndio-dev:amd64 (1.8.1-1.1) ...\n",
            "Selecting previously unselected package libudev-dev:amd64.\n",
            "Preparing to unpack .../29-libudev-dev_249.11-0ubuntu3.12_amd64.deb ...\n",
            "Unpacking libudev-dev:amd64 (249.11-0ubuntu3.12) ...\n",
            "Selecting previously unselected package libxfixes-dev:amd64.\n",
            "Preparing to unpack .../30-libxfixes-dev_1%3a6.0.0-1_amd64.deb ...\n",
            "Unpacking libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Selecting previously unselected package libxcursor-dev:amd64.\n",
            "Preparing to unpack .../31-libxcursor-dev_1%3a1.2.0-2build4_amd64.deb ...\n",
            "Unpacking libxcursor-dev:amd64 (1:1.2.0-2build4) ...\n",
            "Selecting previously unselected package libxi-dev:amd64.\n",
            "Preparing to unpack .../32-libxi-dev_2%3a1.8-1build1_amd64.deb ...\n",
            "Unpacking libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Selecting previously unselected package libxinerama-dev:amd64.\n",
            "Preparing to unpack .../33-libxinerama-dev_2%3a1.1.4-3_amd64.deb ...\n",
            "Unpacking libxinerama-dev:amd64 (2:1.1.4-3) ...\n",
            "Selecting previously unselected package libxkbcommon-dev:amd64.\n",
            "Preparing to unpack .../34-libxkbcommon-dev_1.4.0-1_amd64.deb ...\n",
            "Unpacking libxkbcommon-dev:amd64 (1.4.0-1) ...\n",
            "Selecting previously unselected package libxrandr-dev:amd64.\n",
            "Preparing to unpack .../35-libxrandr-dev_2%3a1.5.2-1build1_amd64.deb ...\n",
            "Unpacking libxrandr-dev:amd64 (2:1.5.2-1build1) ...\n",
            "Selecting previously unselected package libxt-dev:amd64.\n",
            "Preparing to unpack .../36-libxt-dev_1%3a1.2.1-1_amd64.deb ...\n",
            "Unpacking libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Selecting previously unselected package libxv-dev:amd64.\n",
            "Preparing to unpack .../37-libxv-dev_2%3a1.0.11-1build2_amd64.deb ...\n",
            "Unpacking libxv-dev:amd64 (2:1.0.11-1build2) ...\n",
            "Selecting previously unselected package libxxf86vm-dev:amd64.\n",
            "Preparing to unpack .../38-libxxf86vm-dev_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libxxf86vm-dev:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libsdl2-dev:amd64.\n",
            "Preparing to unpack .../39-libsdl2-dev_2.0.20+dfsg-2ubuntu1.22.04.1_amd64.deb ...\n",
            "Unpacking libsdl2-dev:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
            "Selecting previously unselected package libsdl2-image-2.0-0:amd64.\n",
            "Preparing to unpack .../40-libsdl2-image-2.0-0_2.0.5+dfsg1-3build1_amd64.deb ...\n",
            "Unpacking libsdl2-image-2.0-0:amd64 (2.0.5+dfsg1-3build1) ...\n",
            "Selecting previously unselected package libsdl2-image-dev:amd64.\n",
            "Preparing to unpack .../41-libsdl2-image-dev_2.0.5+dfsg1-3build1_amd64.deb ...\n",
            "Unpacking libsdl2-image-dev:amd64 (2.0.5+dfsg1-3build1) ...\n",
            "Selecting previously unselected package libsdl2-mixer-2.0-0:amd64.\n",
            "Preparing to unpack .../42-libsdl2-mixer-2.0-0_2.0.4+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking libsdl2-mixer-2.0-0:amd64 (2.0.4+dfsg1-4build1) ...\n",
            "Selecting previously unselected package libsdl2-mixer-dev:amd64.\n",
            "Preparing to unpack .../43-libsdl2-mixer-dev_2.0.4+dfsg1-4build1_amd64.deb ...\n",
            "Unpacking libsdl2-mixer-dev:amd64 (2.0.4+dfsg1-4build1) ...\n",
            "Selecting previously unselected package libsdl2-ttf-2.0-0:amd64.\n",
            "Preparing to unpack .../44-libsdl2-ttf-2.0-0_2.0.18+dfsg-2_amd64.deb ...\n",
            "Unpacking libsdl2-ttf-2.0-0:amd64 (2.0.18+dfsg-2) ...\n",
            "Selecting previously unselected package libsdl2-ttf-dev:amd64.\n",
            "Preparing to unpack .../45-libsdl2-ttf-dev_2.0.18+dfsg-2_amd64.deb ...\n",
            "Unpacking libsdl2-ttf-dev:amd64 (2.0.18+dfsg-2) ...\n",
            "Setting up libsdl2-image-2.0-0:amd64 (2.0.5+dfsg1-3build1) ...\n",
            "Setting up libmodplug1:amd64 (1:0.8.9.0-3) ...\n",
            "Setting up libsndio-dev:amd64 (1.8.1-1.1) ...\n",
            "Setting up libpciaccess-dev:amd64 (0.16-3) ...\n",
            "Setting up libglvnd-core-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxxf86vm-dev:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up libxkbcommon-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libibus-1.0-5:amd64 (1.5.26-4) ...\n",
            "Setting up libgbm-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libxfixes-dev:amd64 (1:6.0.0-1) ...\n",
            "Setting up libxv-dev:amd64 (2:1.0.11-1build2) ...\n",
            "Setting up libwayland-bin (1.20.0-1ubuntu0.1) ...\n",
            "Setting up libxrandr-dev:amd64 (2:1.5.2-1build1) ...\n",
            "Setting up libdbus-1-dev:amd64 (1.12.20-2ubuntu4.1) ...\n",
            "Setting up libxt-dev:amd64 (1:1.2.1-1) ...\n",
            "Setting up libpulse-mainloop-glib0:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Setting up libgles1:amd64 (1.4.0-1) ...\n",
            "Setting up libudev-dev:amd64 (249.11-0ubuntu3.12) ...\n",
            "Setting up libxinerama-dev:amd64 (2:1.1.4-3) ...\n",
            "Setting up libpulse-dev:amd64 (1:15.99.1+dfsg1-1ubuntu2.2) ...\n",
            "Setting up libglx-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libsdl2-ttf-2.0-0:amd64 (2.0.18+dfsg-2) ...\n",
            "Setting up libglu1-mesa:amd64 (9.0.2-1) ...\n",
            "Setting up libopengl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxi-dev:amd64 (2:1.8-1build1) ...\n",
            "Setting up timgm6mb-soundfont (1.3-5) ...\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf2/default-GM.sf2 (default-GM.sf2) in auto mode\n",
            "update-alternatives: using /usr/share/sounds/sf2/TimGM6mb.sf2 to provide /usr/share/sounds/sf3/default-GM.sf3 (default-GM.sf3) in auto mode\n",
            "Setting up gir1.2-ibus-1.0:amd64 (1.5.26-4) ...\n",
            "Setting up libopusfile0 (0.9+20170913-1.1build1) ...\n",
            "Setting up libinstpatch-1.0-2:amd64 (1.1.6-1) ...\n",
            "Setting up libgl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libfluidsynth3:amd64 (2.2.5-1) ...\n",
            "Setting up libdrm-dev:amd64 (2.4.113-2~ubuntu0.22.04.1) ...\n",
            "Setting up libegl-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libxcursor-dev:amd64 (1:1.2.0-2build4) ...\n",
            "Setting up libwayland-dev:amd64 (1.20.0-1ubuntu0.1) ...\n",
            "Setting up libibus-1.0-dev:amd64 (1.5.26-4) ...\n",
            "Setting up libdecor-0-dev:amd64 (0.1.0-3build1) ...\n",
            "Setting up libsdl2-mixer-2.0-0:amd64 (2.0.4+dfsg1-4build1) ...\n",
            "Setting up libglu1-mesa-dev:amd64 (9.0.2-1) ...\n",
            "Setting up libgles-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libglvnd-dev:amd64 (1.4.0-1) ...\n",
            "Setting up libegl1-mesa-dev:amd64 (23.2.1-1ubuntu3.1~22.04.3) ...\n",
            "Setting up libsdl2-dev:amd64 (2.0.20+dfsg-2ubuntu1.22.04.1) ...\n",
            "Setting up libsdl2-ttf-dev:amd64 (2.0.18+dfsg-2) ...\n",
            "Setting up libsdl2-mixer-dev:amd64 (2.0.4+dfsg1-4build1) ...\n",
            "Setting up libsdl2-image-dev:amd64 (2.0.5+dfsg1-3build1) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n",
            "Collecting pyvirtualdisplay\n",
            "  Downloading PyVirtualDisplay-3.0-py3-none-any.whl.metadata (943 bytes)\n",
            "Downloading PyVirtualDisplay-3.0-py3-none-any.whl (15 kB)\n",
            "Installing collected packages: pyvirtualdisplay\n",
            "Successfully installed pyvirtualdisplay-3.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb # install Xvfb"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DWg5srcVW3cT",
        "outputId": "77fd1f8c-315d-4b7c-b589-f182e3476624"
      },
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common\n",
            "The following NEW packages will be installed:\n",
            "  libfontenc1 libxfont2 libxkbfile1 x11-xkb-utils xfonts-base xfonts-encodings xfonts-utils\n",
            "  xserver-common xvfb\n",
            "0 upgraded, 9 newly installed, 0 to remove and 29 not upgraded.\n",
            "Need to get 7,814 kB of archives.\n",
            "After this operation, 12.0 MB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu jammy/main amd64 libfontenc1 amd64 1:1.1.4-1build3 [14.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxfont2 amd64 1:2.0.5-1build1 [94.5 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu jammy/main amd64 libxkbfile1 amd64 1:1.1.0-1build3 [71.8 kB]\n",
            "Get:4 http://archive.ubuntu.com/ubuntu jammy/main amd64 x11-xkb-utils amd64 7.7+5build4 [172 kB]\n",
            "Get:5 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-encodings all 1:1.0.5-0ubuntu2 [578 kB]\n",
            "Get:6 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-utils amd64 1:7.7+6build2 [94.6 kB]\n",
            "Get:7 http://archive.ubuntu.com/ubuntu jammy/main amd64 xfonts-base all 1:1.0.5 [5,896 kB]\n",
            "Get:8 http://archive.ubuntu.com/ubuntu jammy-updates/main amd64 xserver-common all 2:21.1.4-2ubuntu1.7~22.04.13 [29.1 kB]\n",
            "Get:9 http://archive.ubuntu.com/ubuntu jammy-updates/universe amd64 xvfb amd64 2:21.1.4-2ubuntu1.7~22.04.13 [863 kB]\n",
            "Fetched 7,814 kB in 0s (22.8 MB/s)\n",
            "Selecting previously unselected package libfontenc1:amd64.\n",
            "(Reading database ... 127537 files and directories currently installed.)\n",
            "Preparing to unpack .../0-libfontenc1_1%3a1.1.4-1build3_amd64.deb ...\n",
            "Unpacking libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Selecting previously unselected package libxfont2:amd64.\n",
            "Preparing to unpack .../1-libxfont2_1%3a2.0.5-1build1_amd64.deb ...\n",
            "Unpacking libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Selecting previously unselected package libxkbfile1:amd64.\n",
            "Preparing to unpack .../2-libxkbfile1_1%3a1.1.0-1build3_amd64.deb ...\n",
            "Unpacking libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Selecting previously unselected package x11-xkb-utils.\n",
            "Preparing to unpack .../3-x11-xkb-utils_7.7+5build4_amd64.deb ...\n",
            "Unpacking x11-xkb-utils (7.7+5build4) ...\n",
            "Selecting previously unselected package xfonts-encodings.\n",
            "Preparing to unpack .../4-xfonts-encodings_1%3a1.0.5-0ubuntu2_all.deb ...\n",
            "Unpacking xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Selecting previously unselected package xfonts-utils.\n",
            "Preparing to unpack .../5-xfonts-utils_1%3a7.7+6build2_amd64.deb ...\n",
            "Unpacking xfonts-utils (1:7.7+6build2) ...\n",
            "Selecting previously unselected package xfonts-base.\n",
            "Preparing to unpack .../6-xfonts-base_1%3a1.0.5_all.deb ...\n",
            "Unpacking xfonts-base (1:1.0.5) ...\n",
            "Selecting previously unselected package xserver-common.\n",
            "Preparing to unpack .../7-xserver-common_2%3a21.1.4-2ubuntu1.7~22.04.13_all.deb ...\n",
            "Unpacking xserver-common (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../8-xvfb_2%3a21.1.4-2ubuntu1.7~22.04.13_amd64.deb ...\n",
            "Unpacking xvfb (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Setting up libfontenc1:amd64 (1:1.1.4-1build3) ...\n",
            "Setting up xfonts-encodings (1:1.0.5-0ubuntu2) ...\n",
            "Setting up libxkbfile1:amd64 (1:1.1.0-1build3) ...\n",
            "Setting up libxfont2:amd64 (1:2.0.5-1build1) ...\n",
            "Setting up x11-xkb-utils (7.7+5build4) ...\n",
            "Setting up xfonts-utils (1:7.7+6build2) ...\n",
            "Setting up xfonts-base (1:1.0.5) ...\n",
            "Setting up xserver-common (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Setting up xvfb (2:21.1.4-2ubuntu1.7~22.04.13) ...\n",
            "Processing triggers for man-db (2.10.2-1) ...\n",
            "Processing triggers for fontconfig (2.13.1-4.2ubuntu5) ...\n",
            "Processing triggers for libc-bin (2.35-0ubuntu3.8) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_5.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind_2_0.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libhwloc.so.15 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbbind.so.3 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbbmalloc_proxy.so.2 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libumf.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_loader.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtbb.so.12 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libtcm_debug.so.1 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_level_zero.so.0 is not a symbolic link\n",
            "\n",
            "/sbin/ldconfig.real: /usr/local/lib/libur_adapter_opencl.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import time\n",
        "import ale_py\n",
        "from stable_baselines3 import DQN\n",
        "from stable_baselines3.common.atari_wrappers import AtariWrapper\n",
        "from gymnasium.wrappers import RecordVideo  # Import the video recorder wrapper\n",
        "from pyvirtualdisplay import Display # Import Display from pyvirtualdisplay\n",
        "\n",
        "# Initialize virtual display\n",
        "display = Display(visible=0, size=(1400, 900))\n",
        "display.start()\n",
        "\n",
        "# ------------------------------\n",
        "# Load the Trained Model\n",
        "# ------------------------------\n",
        "model = DQN.load(\"/content/drive/My Drive/deep_q_learning/dqn_model.zip\")\n",
        "\n",
        "# ------------------------------\n",
        "# Environment Setup for Evaluation with Video Recording\n",
        "# ------------------------------\n",
        "env_id = \"ALE/Boxing-v5\"\n",
        "# Create the environment with render_mode \"rgb_array\" to capture frames.\n",
        "env = gym.make(env_id, render_mode=\"rgb_array\")\n",
        "env = AtariWrapper(env)\n",
        "\n",
        "# Wrap the environment with RecordVideo to record every episode.\n",
        "# The video files will be saved in the \"videos/\" folder.\n",
        "env = RecordVideo(env, video_folder=\"/content/drive/My Drive/deep_q_learning/videos/\", episode_trigger=lambda episode_id: True)\n",
        "\n",
        "# For visualizing the game in real-time, we create a separate environment instance for rendering.\n",
        "render_env = gym.make(env_id, render_mode=\"human\")\n",
        "render_env = AtariWrapper(render_env)\n",
        "\n",
        "# ------------------------------\n",
        "# Playing with the Agent using a Greedy Policy\n",
        "# ------------------------------\n",
        "episodes = 5  # Number of evaluation episodes\n",
        "\n",
        "for ep in range(episodes):\n",
        "    obs, info = env.reset()\n",
        "    render_env.reset()  # Reset the render environment as well\n",
        "    done = False\n",
        "    episode_reward = 0\n",
        "    while not done:\n",
        "        # Choose the best action deterministically (greedy)\n",
        "        action, _states = model.predict(obs, deterministic=True)\n",
        "        obs, reward, done, truncated, info = env.step(action)\n",
        "        episode_reward += reward\n",
        "\n",
        "        # Render the environment in real time (from the separate render environment)\n",
        "        # Correct the unpacking to handle all returned values from step\n",
        "        render_obs, render_reward, render_terminated, render_truncated, render_info = render_env.step(action)\n",
        "        time.sleep(0.02)  # Slow down for visualization\n",
        "\n",
        "        if done or truncated:\n",
        "            break\n",
        "    print(f\"Episode {ep+1} Reward: {episode_reward}\")\n",
        "\n",
        "env.close()\n",
        "render_env.close()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O74FyblhUbH0",
        "outputId": "9c1ce7fa-e549-4d8d-eae1-cf6c1ae5584c"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/gymnasium/wrappers/rendering.py:283: UserWarning: \u001b[33mWARN: Overwriting existing videos at /content/drive/My Drive/deep_q_learning/videos folder (try specifying a different `video_folder` for the `RecordVideo` wrapper if this is not desired)\u001b[0m\n",
            "  logger.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 1 Reward: 9.0\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/config_defaults.py:1: DeprecationWarning: invalid escape sequence '\\P'\n",
            "  \"\"\"\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Episode 2 Reward: 16.0\n",
            "Episode 3 Reward: 8.0\n",
            "Episode 4 Reward: 31.0\n",
            "Episode 5 Reward: 6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Merge Videos"
      ],
      "metadata": {
        "id": "acU52ZDdam1d"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install moviepy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KEVtPFhqash_",
        "outputId": "fe2644a8-8507-4ee1-8fa2-f5ce15f96bcd"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: moviepy in /usr/local/lib/python3.11/dist-packages (1.0.3)\n",
            "Requirement already satisfied: decorator<5.0,>=4.0.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.4.2)\n",
            "Requirement already satisfied: imageio<3.0,>=2.5 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.37.0)\n",
            "Requirement already satisfied: imageio_ffmpeg>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.6.0)\n",
            "Requirement already satisfied: tqdm<5.0,>=4.11.2 in /usr/local/lib/python3.11/dist-packages (from moviepy) (4.67.1)\n",
            "Requirement already satisfied: numpy>=1.17.3 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.0.2)\n",
            "Requirement already satisfied: requests<3.0,>=2.8.1 in /usr/local/lib/python3.11/dist-packages (from moviepy) (2.32.3)\n",
            "Requirement already satisfied: proglog<=1.0.0 in /usr/local/lib/python3.11/dist-packages (from moviepy) (0.1.10)\n",
            "Requirement already satisfied: pillow>=8.3.2 in /usr/local/lib/python3.11/dist-packages (from imageio<3.0,>=2.5->moviepy) (11.1.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.4.1)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests<3.0,>=2.8.1->moviepy) (2025.1.31)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from moviepy.editor import VideoFileClip, concatenate_videoclips\n",
        "\n",
        "def merge_videos(video_folder, output_path):\n",
        "    \"\"\"\n",
        "    Merges all video files in a folder into a single output video.\n",
        "\n",
        "    Args:\n",
        "        video_folder: Path to the folder containing the video files.\n",
        "        output_path: Path to save the merged video file.\n",
        "    \"\"\"\n",
        "    video_files = [f for f in os.listdir(video_folder) if f.endswith(('.mp4', '.avi'))]  # Add other supported formats if needed\n",
        "    video_files.sort()  # Sort files to maintain order\n",
        "    clips = [VideoFileClip(os.path.join(video_folder, f)) for f in video_files]\n",
        "    final_clip = concatenate_videoclips(clips)\n",
        "    final_clip.write_videofile(output_path, codec=\"libx264\", fps=24)  # Adjust codec and fps as needed\n",
        "\n",
        "# Example usage:\n",
        "video_folder = \"/content/drive/My Drive/deep_q_learning/videos/\"  # Path to your video folder\n",
        "output_path = \"/content/drive/My Drive/deep_q_learning/merged_video.mp4\"  # Path to save the merged video\n",
        "merge_videos(video_folder, output_path)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0Wv3zBPJazon",
        "outputId": "f0a38e30-38ba-4e56-97b5-77803744fcc8"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/moviepy/video/io/ffmpeg_reader.py:294: DeprecationWarning: invalid escape sequence '\\d'\n",
            "  lines_video = [l for l in lines if ' Video: ' in l and re.search('\\d+x\\d+', l)]\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/fx/painting.py:7: DeprecationWarning: Please import `sobel` from the `scipy.ndimage` namespace; the `scipy.ndimage.filters` namespace is deprecated and will be removed in SciPy 2.0.0.\n",
            "  from scipy.ndimage.filters import sobel\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/moviepy/video/io/sliders.py:61: SyntaxWarning: \"is\" with a literal. Did you mean \"==\"?\n",
            "  if event.key is 'enter':\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n",
            "WARNING:py.warnings:/usr/local/lib/python3.11/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Building video /content/drive/My Drive/deep_q_learning/merged_video.mp4.\n",
            "Moviepy - Writing video /content/drive/My Drive/deep_q_learning/merged_video.mp4\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "                                                                  "
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Moviepy - Done !\n",
            "Moviepy - video ready /content/drive/My Drive/deep_q_learning/merged_video.mp4\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r"
          ]
        }
      ]
    }
  ]
}